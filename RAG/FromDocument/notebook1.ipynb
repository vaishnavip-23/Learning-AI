{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fef45c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install langchain langchain-google-genai chromadb python-dotenv unstructured sentence-transformers langchain-community --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecd407e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and load API key\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and set it in the environment\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY in your .env file\")\n",
    "\n",
    "# Set the API key for Google Generative AI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50220ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16fffda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "# Set up paths with absolute references\n",
    "import os\n",
    "\n",
    "# Get absolute paths\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_PATH = os.path.join(current_dir, \"alice_in_wonderland.md\")\n",
    "PERSIST_DIR = os.path.join(current_dir, \"chroma_rag_db\")  # New database directory\n",
    "COLLECTION = \"alice_wonderland\"\n",
    "\n",
    "# Ensure the persist directory exists with proper permissions\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "os.chmod(PERSIST_DIR, 0o755)  # More secure permissions (rwxr-xr-x)\n",
    "\n",
    "# Load and preprocess the document\n",
    "loader = UnstructuredMarkdownLoader(DATA_PATH, show_progress=True)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Configure text splitter for better chunks\n",
    "CHUNK_SIZE = 1000  # Larger chunks to maintain more context\n",
    "CHUNK_OVERLAP = 200  # Larger overlap to prevent losing context at boundaries\n",
    "\n",
    "# Use RecursiveCharacterTextSplitter with better separators\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],  # More granular splitting\n",
    "    keep_separator=True,  # Keep the separators to maintain readability\n",
    "    strip_whitespace=True,  # Clean up whitespace\n",
    "    add_start_index=True,  # Add position info to metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec91d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 190 chunks, 185 after filtering\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks and filter out boilerplate\n",
    "def is_meaningful_chunk(text: str) -> bool:\n",
    "    # Skip headers, licensing info, and other boilerplate\n",
    "    skip_patterns = [\n",
    "        \"Project Gutenberg\",\n",
    "        \"THE MILLENNIUM FULCRUM EDITION\",\n",
    "        \"Contents\",\n",
    "        \"*      *      *\",\n",
    "        \"trademark\",\n",
    "        \"license\",\n",
    "        \"copyright\"\n",
    "    ]\n",
    "    return not any(pattern.lower() in text.lower() for pattern in skip_patterns)\n",
    "\n",
    "# Split and filter chunks\n",
    "chunks = splitter.split_documents(docs)\n",
    "filtered_chunks = [\n",
    "    chunk for chunk in chunks \n",
    "    if is_meaningful_chunk(chunk.page_content) and len(chunk.page_content.strip()) > 50  # Skip very short chunks\n",
    "]\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks, {len(filtered_chunks)} after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efaf1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing collection: alice_wonderland\n",
      "Database created and saved to disk with 185 chunks\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings & persist vector DB\n",
    "\n",
    "# Initialize ChromaDB with explicit settings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB client with explicit settings\n",
    "chroma_settings = Settings(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    is_persistent=True,\n",
    "    anonymized_telemetry=False\n",
    ")\n",
    "\n",
    "# Create a new client instance\n",
    "chroma_client = chromadb.Client(chroma_settings)\n",
    "\n",
    "# Get or create collection - this is safer than deleting/recreating\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=COLLECTION)\n",
    "    print(f\"Using existing collection: {COLLECTION}\")\n",
    "except:\n",
    "    collection = chroma_client.create_collection(name=COLLECTION)\n",
    "    print(f\"Created new collection: {COLLECTION}\")\n",
    "\n",
    "# Initialize the sentence-transformers embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Lightweight, fast model\n",
    "model_kwargs = {\n",
    "    'device': 'cpu'  # Use CPU for better compatibility\n",
    "}\n",
    "encode_kwargs = {\n",
    "    'normalize_embeddings': True,  # Normalize for better similarity matching\n",
    "    'batch_size': 32  # Process in smaller batches for memory efficiency\n",
    "}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Create vector DB with filtered chunks\n",
    "# Note: Using normalized embeddings which automatically uses cosine similarity\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=filtered_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    collection_name=COLLECTION,\n",
    "    client=chroma_client  # Use our explicitly configured client\n",
    ")\n",
    "print(f\"Database created and saved to disk with {len(filtered_chunks)} chunks\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f85ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Describe the Queen of Hearts?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: Describe the Queen of Hearts?\n",
      "\n",
      "Retrieved 1 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "First came ten soldiers carrying clubs; these were all shaped like the three gardeners, oblong and flat, with their hands and feet at the corners: next the ten courtiers; these were ornamented all over with diamonds, and walked two and two, as the soldiers did. After these came the royal children; there were ten of them, and the little dears came jumping merrily along hand in hand, in couples: they were all ornamented with hearts. Next came the guests, mostly Kings and Queens, and among them Alice recognised the White Rabbit: it was talking in a hurried nervous manner, smiling at everything that was said, and went by without noticing her. Then followed the Knave of Hearts, carrying the Kingâ€™s crown on a crimson velvet cushion; and, last of all this grand procession, came THE KING AND QUEEN OF HEARTS.\n",
      "\n",
      "Metadata: {'start_index': 86630, 'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "Based on the provided context, the Queen of Hearts is mentioned as the last participant in a grand procession, arriving after the Knave of Hearts who carried the King's crown. The text states, \"last of all this grand procession, came THE KING AND QUEEN OF HEARTS.\" The context does not provide any further description of her appearance, attire, or behavior.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reload persisted DB with consistent settings\n",
    "vectordb = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION,  # Use same collection name\n",
    "    client_settings=chroma_settings  # Use same settings\n",
    ")\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def answer_query(query: str, k: int = 3):\n",
    "    print(\"=== RETRIEVAL STEP ===\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Use MMR for better diversity and relevance\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": k,\n",
    "            \"fetch_k\": k * 3,\n",
    "            \"lambda_mult\": 0.7  # Balance between relevance (1.0) and diversity (0.0)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get initial documents\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Filter for unique content and those with start_index\n",
    "    seen_content = set()\n",
    "    filtered_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Get start_index from metadata if it exists\n",
    "        start_index = doc.metadata.get('start_index', None)\n",
    "        if start_index is None:\n",
    "            continue\n",
    "            \n",
    "        # Normalize content for comparison (remove extra whitespace)\n",
    "        content = ' '.join(doc.page_content.split())\n",
    "        \n",
    "        # Skip if we've seen this content before\n",
    "        if content in seen_content:\n",
    "            continue\n",
    "            \n",
    "        seen_content.add(content)\n",
    "        filtered_docs.append((doc, start_index))\n",
    "    \n",
    "    # Sort by start_index and take top 3\n",
    "    filtered_docs.sort(key=lambda x: x[1])\n",
    "    final_docs = [doc for doc, _ in filtered_docs[:3]]\n",
    "    \n",
    "    # Show retrieved chunks\n",
    "    print(f\"Retrieved {len(final_docs)} unique chunks:\")\n",
    "    for i, doc in enumerate(final_docs):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(\"Content:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"\\nMetadata:\", doc.metadata)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if not final_docs:\n",
    "        print(\"\\nNo relevant chunks found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== GENERATION STEP ===\")\n",
    "    \n",
    "    # Optimized prompt for flash model with narrative context\n",
    "    template = \"\"\"You are helping answer questions about Alice in Wonderland. Use only the provided context to answer. Be detailed in your answer and don't say you can't answer the question if there's retrieved chunks. Summarize the chunks very well.\n",
    "If you can't find the answer in the context, say \"Based on the provided context, I cannot answer this question.\"\n",
    "\n",
    "Context (in story order):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Use only information from the context\n",
    "2. Be specific and quote relevant parts\n",
    "3. Follow the story's sequence when describing events\n",
    "4. If information is incomplete, say so\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Format prompt with better context joining and position info\n",
    "    contexts = []\n",
    "    for doc in final_docs:\n",
    "        # Add position context to help with narrative flow\n",
    "        start_idx = doc.metadata.get('start_index', 0)\n",
    "        context = f\"[Story position {start_idx}]:\\n{doc.page_content}\"\n",
    "        contexts.append(context)\n",
    "    formatted_context = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "    formatted = prompt.format(context=formatted_context, question=query)\n",
    "\n",
    "    # Use Gemini flash model with narrative-optimized settings\n",
    "    chat = ChatGoogleGenerativeAI(\n",
    "        model=\"models/gemini-2.5-flash\",\n",
    "        temperature=0.2,\n",
    "        top_p=0.85,\n",
    "        top_k=30,\n",
    "        max_output_tokens=512\n",
    "    )\n",
    "    response = chat.invoke(formatted)\n",
    "\n",
    "    # Robust final answer printing with fallback if model returns empty\n",
    "    answer_text = \"\"\n",
    "    try:\n",
    "        answer_text = (response.content or \"\").strip()\n",
    "    except Exception:\n",
    "        answer_text = \"\"\n",
    "\n",
    "    if not answer_text:\n",
    "        # Fallback: concise extractive summary from retrieved chunks\n",
    "        snippet_texts = []\n",
    "        for doc in final_docs:\n",
    "            text = (doc.page_content or \"\").strip()\n",
    "            if text:\n",
    "                snippet_texts.append(text)\n",
    "        fallback = \" \".join(snippet_texts)\n",
    "        # Keep fallback short-ish to avoid overly long outputs\n",
    "        fallback = (fallback[:900] + \"...\") if len(fallback) > 900 else fallback\n",
    "        answer_text = \"(LLM response unavailable; using extractive summary from retrieved context)\\n\" + fallback\n",
    "\n",
    "    print(\"\\n=== FINAL ANSWER ===\")\n",
    "    print(answer_text)\n",
    "\n",
    "\n",
    "# Test queries focusing on specific events/characters\n",
    "queries = [\n",
    "    \"Describe the Queen of Hearts?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    answer_query(query)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
