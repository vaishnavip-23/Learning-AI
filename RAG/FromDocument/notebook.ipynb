{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9fef45c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755666339.657884 4712835 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install langchain langchain-google-genai chromadb python-dotenv unstructured sentence-transformers langchain-community --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ecd407e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and load API key\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and set it in the environment\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY in your .env file\")\n",
    "\n",
    "# Set the API key for Google Generative AI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "50220ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "16fffda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"alice_in_wonderland.md\"  \n",
    "PERSIST_DIR = \"chroma_db\"  \n",
    "COLLECTION = \"docs\"\n",
    "\n",
    "# Load and preprocess the document\n",
    "loader = UnstructuredMarkdownLoader(DATA_PATH, show_progress=True)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Configure text splitter for better chunks\n",
    "CHUNK_SIZE = 1000  # Larger chunks to maintain more context\n",
    "CHUNK_OVERLAP = 200  # Larger overlap to prevent losing context at boundaries\n",
    "\n",
    "# Use RecursiveCharacterTextSplitter with better separators\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],  # More granular splitting\n",
    "    keep_separator=True,  # Keep the separators to maintain readability\n",
    "    strip_whitespace=True,  # Clean up whitespace\n",
    "    add_start_index=True,  # Add position info to metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ec91d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 217 chunks, 185 after filtering\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks and filter out boilerplate\n",
    "def is_meaningful_chunk(text: str) -> bool:\n",
    "    # Skip headers, licensing info, and other boilerplate\n",
    "    skip_patterns = [\n",
    "        \"Project Gutenberg\",\n",
    "        \"THE MILLENNIUM FULCRUM EDITION\",\n",
    "        \"Contents\",\n",
    "        \"*      *      *\",\n",
    "        \"trademark\",\n",
    "        \"license\",\n",
    "        \"copyright\"\n",
    "    ]\n",
    "    return not any(pattern.lower() in text.lower() for pattern in skip_patterns)\n",
    "\n",
    "# Split and filter chunks\n",
    "chunks = splitter.split_documents(docs)\n",
    "filtered_chunks = [\n",
    "    chunk for chunk in chunks \n",
    "    if is_meaningful_chunk(chunk.page_content) and len(chunk.page_content.strip()) > 50  # Skip very short chunks\n",
    "]\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks, {len(filtered_chunks)} after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40e3b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Collection expecting embedding with dimension of 768, got 384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[164], line 27\u001b[0m\n",
      "\u001b[1;32m     19\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(\n",
      "\u001b[1;32m     20\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n",
      "\u001b[1;32m     21\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n",
      "\u001b[1;32m     22\u001b[0m     encode_kwargs\u001b[38;5;241m=\u001b[39mencode_kwargs\n",
      "\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create vector DB with filtered chunks\u001b[39;00m\n",
      "\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Note: Using normalized embeddings which automatically uses cosine similarity\u001b[39;00m\n",
      "\u001b[0;32m---> 27\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiltered_chunks\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPERSIST_DIR\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOLLECTION\u001b[49m\n",
      "\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatabase created and saved to disk with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:887\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    885\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n",
      "\u001b[1;32m    886\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n",
      "\u001b[0;32m--> 887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:843\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_batches\n",
      "\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n",
      "\u001b[1;32m    838\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,\n",
      "\u001b[1;32m    839\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n",
      "\u001b[1;32m    840\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,\n",
      "\u001b[1;32m    841\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n",
      "\u001b[1;32m    842\u001b[0m     ):\n",
      "\u001b[0;32m--> 843\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    849\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:299\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    297\u001b[0m ids_with_metadata \u001b[38;5;241m=\u001b[39m [ids[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids]\n",
      "\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_with_metadata\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected metadata value to be\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/chromadb/api/models/Collection.py:374\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n",
      "\u001b[1;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n",
      "\u001b[1;32m    355\u001b[0m \n",
      "\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n",
      "\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    365\u001b[0m upsert_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_prepare_upsert_request(\n",
      "\u001b[1;32m    366\u001b[0m     ids\u001b[38;5;241m=\u001b[39mids,\n",
      "\u001b[1;32m    367\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    371\u001b[0m     uris\u001b[38;5;241m=\u001b[39muris,\n",
      "\u001b[1;32m    372\u001b[0m )\n",
      "\u001b[0;32m--> 374\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsert\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsert_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsert_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsert_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadatas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsert_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupsert_request\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muris\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/chromadb/api/rust.py:464\u001b[0m, in \u001b[0;36mRustBindingsAPI._upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris, tenant, database)\u001b[0m\n",
      "\u001b[1;32m    452\u001b[0m \u001b[38;5;129m@override\u001b[39m\n",
      "\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upsert\u001b[39m(\n",
      "\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    462\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n",
      "\u001b[1;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbindings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Collection expecting embedding with dimension of 768, got 384"
     ]
    }
   ],
   "source": [
    "# Create embeddings & persist vector DB\n",
    "\n",
    "# Clean up existing database if it exists\n",
    "import shutil\n",
    "if os.path.exists(PERSIST_DIR):\n",
    "    shutil.rmtree(PERSIST_DIR)\n",
    "    print(f\"Cleaned up existing database at {PERSIST_DIR}\")\n",
    "\n",
    "# Initialize the sentence-transformers embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Lightweight, fast model\n",
    "model_kwargs = {\n",
    "    'device': 'cpu'  # Use CPU for better compatibility\n",
    "}\n",
    "encode_kwargs = {\n",
    "    'normalize_embeddings': True,  # Normalize for better similarity matching\n",
    "    'batch_size': 32  # Process in smaller batches for memory efficiency\n",
    "}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Create vector DB with filtered chunks\n",
    "# Note: Using normalized embeddings which automatically uses cosine similarity\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=filtered_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    collection_name=COLLECTION\n",
    ")\n",
    "print(f\"Database created and saved to disk with {len(filtered_chunks)} chunks\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What happens at the tea party?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: What happens at the tea party?\n",
      "\n",
      "Retrieved 0 unique chunks:\n",
      "\n",
      "No relevant chunks found.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: How does Alice meet the Mad Hatter?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: How does Alice meet the Mad Hatter?\n",
      "\n",
      "Retrieved 3 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "“Oh, you’re sure to do that,” said the Cat, “if you only walk long enough.”\n",
      "\n",
      "Alice felt that this could not be denied, so she tried another question. “What sort of people live about here?”\n",
      "\n",
      "“In that direction,” the Cat said, waving its right paw round, “lives a Hatter: and in that direction,” waving the other paw, “lives a March Hare. Visit either you like: they’re both mad.”\n",
      "\n",
      "“But I don’t want to go among mad people,” Alice remarked.\n",
      "\n",
      "“Oh, you can’t help that,” said the Cat: “we’re all mad here. I’m mad. You’re mad.”\n",
      "\n",
      "“How do you know I’m mad?” said Alice.\n",
      "\n",
      "“You must be,” said the Cat, “or you wouldn’t have come here.”\n",
      "\n",
      "Alice didn’t think that proved it at all; however, she went on “And how do you know that you’re mad?”\n",
      "\n",
      "“To begin with,” said the Cat, “a dog’s not mad. You grant that?”\n",
      "\n",
      "“I suppose so,” said Alice.\n",
      "\n",
      "Metadata: {'start_index': 68988, 'source': 'alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "“Well! I’ve often seen a cat without a grin,” thought Alice; “but a grin without a cat! It’s the most curious thing I ever saw in my life!”\n",
      "\n",
      "She had not gone much farther before she came in sight of the house of the March Hare: she thought it must be the right house, because the chimneys were shaped like ears and the roof was thatched with fur. It was so large a house, that she did not like to go nearer till she had nibbled some more of the lefthand bit of mushroom, and raised herself to about two feet high: even then she walked up towards it rather timidly, saying to herself “Suppose it should be raving mad after all! I almost wish I’d gone to see the Hatter instead!”\n",
      "\n",
      "CHAPTER VII. A Mad Tea-Party\n",
      "\n",
      "Metadata: {'source': 'alice_in_wonderland.md', 'start_index': 71488}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content:\n",
      "“Well, I’d hardly finished the first verse,” said the Hatter, “when the Queen jumped up and bawled out, ‘He’s murdering the time! Off with his head!’”\n",
      "\n",
      "“How dreadfully savage!” exclaimed Alice.\n",
      "\n",
      "“And ever since that,” the Hatter went on in a mournful tone, “he won’t do a thing I ask! It’s always six o’clock now.”\n",
      "\n",
      "A bright idea came into Alice’s head. “Is that the reason so many tea-things are put out here?” she asked.\n",
      "\n",
      "“Yes, that’s it,” said the Hatter with a sigh: “it’s always tea-time, and we’ve no time to wash the things between whiles.”\n",
      "\n",
      "“Then you keep moving round, I suppose?” said Alice.\n",
      "\n",
      "“Exactly so,” said the Hatter: “as the things get used up.”\n",
      "\n",
      "“But what happens when you come to the beginning again?” Alice ventured to ask.\n",
      "\n",
      "“Suppose we change the subject,” the March Hare interrupted, yawning. “I’m getting tired of this. I vote the young lady tells us a story.”\n",
      "\n",
      "“I’m afraid I don’t know one,” said Alice, rather alarmed at the proposal.\n",
      "\n",
      "Metadata: {'source': 'alice_in_wonderland.md', 'start_index': 78522}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "Based on the provided context, Alice learns about the Mad Hatter from the Cheshire Cat, who says,  “In that direction,” the Cat said, waving its right paw round, “lives a Hatter: and in that direction,” waving the other paw, “lives a March Hare. Visit either you like: they’re both mad.”  Later, Alice sees the March Hare's house, which is described as having chimneys shaped like ears and a fur-thatched roof, and considers visiting the Hatter instead.  The provided text then jumps to a Mad Tea-Party with the Hatter and March Hare, but doesn't describe the actual meeting between Alice and the Hatter.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: What does the Queen of Hearts say?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: What does the Queen of Hearts say?\n",
      "\n",
      "Retrieved 1 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "Alice was rather doubtful whether she ought not to lie down on her face like the three gardeners, but she could not remember ever having heard of such a rule at processions; “and besides, what would be the use of a procession,” thought she, “if people had all to lie down upon their faces, so that they couldn’t see it?” So she stood still where she was, and waited.\n",
      "\n",
      "When the procession came opposite to Alice, they all stopped and looked at her, and the Queen said severely “Who is this?” She said it to the Knave of Hearts, who only bowed and smiled in reply.\n",
      "\n",
      "“Idiot!” said the Queen, tossing her head impatiently; and, turning to Alice, she went on, “What’s your name, child?”\n",
      "\n",
      "“My name is Alice, so please your Majesty,” said Alice very politely; but she added, to herself, “Why, they’re only a pack of cards, after all. I needn’t be afraid of them!”\n",
      "\n",
      "Metadata: {'source': 'alice_in_wonderland.md', 'start_index': 87443}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "The Queen of Hearts first says, “Who is this?”  Then, after the Knave of Hearts only bows and smiles, she says, “Idiot!”  Finally, she asks Alice, “What’s your name, child?”\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: Describe the Cheshire Cat's appearance\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: Describe the Cheshire Cat's appearance\n",
      "\n",
      "Retrieved 3 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "Alice was just beginning to think to herself, “Now, what am I to do with this creature when I get it home?” when it grunted again, so violently, that she looked down into its face in some alarm. This time there could be no mistake about it: it was neither more nor less than a pig, and she felt that it would be quite absurd for her to carry it further.\n",
      "\n",
      "So she set the little creature down, and felt quite relieved to see it trot away quietly into the wood. “If it had grown up,” she said to herself, “it would have made a dreadfully ugly child: but it makes rather a handsome pig, I think.” And she began thinking over other children she knew, who might do very well as pigs, and was just saying to herself, “if one only knew the right way to change them—” when she was a little startled by seeing the Cheshire Cat sitting on a bough of a tree a few yards off.\n",
      "\n",
      "Metadata: {'start_index': 67437, 'source': 'alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "“Well! I’ve often seen a cat without a grin,” thought Alice; “but a grin without a cat! It’s the most curious thing I ever saw in my life!”\n",
      "\n",
      "She had not gone much farther before she came in sight of the house of the March Hare: she thought it must be the right house, because the chimneys were shaped like ears and the roof was thatched with fur. It was so large a house, that she did not like to go nearer till she had nibbled some more of the lefthand bit of mushroom, and raised herself to about two feet high: even then she walked up towards it rather timidly, saying to herself “Suppose it should be raving mad after all! I almost wish I’d gone to see the Hatter instead!”\n",
      "\n",
      "CHAPTER VII. A Mad Tea-Party\n",
      "\n",
      "Metadata: {'start_index': 71488, 'source': 'alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content:\n",
      "The players all played at once without waiting for turns, quarrelling all the while, and fighting for the hedgehogs; and in a very short time the Queen was in a furious passion, and went stamping about, and shouting “Off with his head!” or “Off with her head!” about once in a minute.\n",
      "\n",
      "Alice began to feel very uneasy: to be sure, she had not as yet had any dispute with the Queen, but she knew that it might happen any minute, “and then,” thought she, “what would become of me? They’re dreadfully fond of beheading people here; the great wonder is, that there’s any one left alive!”\n",
      "\n",
      "She was looking about for some way of escape, and wondering whether she could get away without being seen, when she noticed a curious appearance in the air: it puzzled her very much at first, but, after watching it a minute or two, she made it out to be a grin, and she said to herself “It’s the Cheshire Cat: now I shall have somebody to talk to.”\n",
      "\n",
      "Metadata: {'start_index': 92640, 'source': 'alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "The Cheshire Cat's first appearance is described as sitting \"on a bough of a tree a few yards off\".  Later, Alice observes \"a curious appearance in the air\" which she identifies as \"a grin\", stating \"It’s the Cheshire Cat\".  Based on the provided context, I cannot fully describe the Cheshire Cat's appearance beyond this grin appearing in the air, separate from its body.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reload persisted DB\n",
    "vectordb = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"docs\"\n",
    ")\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def answer_query(query: str, k: int = 3):\n",
    "    print(\"=== RETRIEVAL STEP ===\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Use MMR for better diversity and relevance\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": k,\n",
    "            \"fetch_k\": k * 3,\n",
    "            \"lambda_mult\": 0.7  # Balance between relevance (1.0) and diversity (0.0)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get initial documents\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Filter for unique content and those with start_index\n",
    "    seen_content = set()\n",
    "    filtered_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Get start_index from metadata if it exists\n",
    "        start_index = doc.metadata.get('start_index', None)\n",
    "        if start_index is None:\n",
    "            continue\n",
    "            \n",
    "        # Normalize content for comparison (remove extra whitespace)\n",
    "        content = ' '.join(doc.page_content.split())\n",
    "        \n",
    "        # Skip if we've seen this content before\n",
    "        if content in seen_content:\n",
    "            continue\n",
    "            \n",
    "        seen_content.add(content)\n",
    "        filtered_docs.append((doc, start_index))\n",
    "    \n",
    "    # Sort by start_index and take top 3\n",
    "    filtered_docs.sort(key=lambda x: x[1])\n",
    "    final_docs = [doc for doc, _ in filtered_docs[:3]]\n",
    "    \n",
    "    # Show retrieved chunks\n",
    "    print(f\"Retrieved {len(final_docs)} unique chunks:\")\n",
    "    for i, doc in enumerate(final_docs):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(\"Content:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"\\nMetadata:\", doc.metadata)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if not final_docs:\n",
    "        print(\"\\nNo relevant chunks found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== GENERATION STEP ===\")\n",
    "    \n",
    "    # Optimized prompt for flash model with narrative context\n",
    "    template = \"\"\"You are helping answer questions about Alice in Wonderland. Use only the provided context to answer.\n",
    "If you can't find the answer in the context, say \"Based on the provided context, I cannot answer this question.\"\n",
    "\n",
    "Context (in story order):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Use only information from the context\n",
    "2. Be specific and quote relevant parts\n",
    "3. Follow the story's sequence when describing events\n",
    "4. If information is incomplete, say so\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Format prompt with better context joining and position info\n",
    "    contexts = []\n",
    "    for doc in final_docs:\n",
    "        # Add position context to help with narrative flow\n",
    "        start_idx = doc.metadata.get('start_index', 0)\n",
    "        context = f\"[Story position {start_idx}]:\\n{doc.page_content}\"\n",
    "        contexts.append(context)\n",
    "    formatted_context = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "    formatted = prompt.format(context=formatted_context, question=query)\n",
    "\n",
    "    # Use Gemini flash model with narrative-optimized settings\n",
    "    chat = ChatGoogleGenerativeAI(\n",
    "        model=\"models/gemini-1.5-flash\",\n",
    "        temperature=0.2,  # Slightly higher for better narrative flow\n",
    "        top_p=0.85,      # More focused token selection\n",
    "        top_k=30,        # More focused selection\n",
    "        max_output_tokens=512  # Limit length for more concise answers\n",
    "    )\n",
    "    response = chat.invoke(formatted)\n",
    "\n",
    "    print(\"\\n=== FINAL ANSWER ===\")\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "# Test queries focusing on specific events/characters\n",
    "queries = [\n",
    "    \"What happens at the tea party?\",\n",
    "    \"How does Alice meet the Mad Hatter?\",\n",
    "    \"What does the Queen of Hearts say?\",\n",
    "    \"Describe the Cheshire Cat's appearance\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    answer_query(query)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
