{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "34563fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade langchain langchain-community langchain-google-genai langchain-text-splitters langchain-huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "b696f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade chromadb sentence-transformers FlagEmbedding huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "e100e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "530d3636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from FlagEmbedding import FlagReranker  # local free reranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "f12c6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "# Set up Hugging Face token\n",
    "import huggingface_hub\n",
    "\n",
    "# Get token from environment variable\n",
    "HF_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Please set the HUGGINGFACE_TOKEN environment variable\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "huggingface_hub.login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "3b167858",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"alice_in_wonderland.md\"\n",
    "PERSIST_DIR = None  # Use in-memory database instead\n",
    "GOOGLE_API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "6d95631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Path(DATA_PATH).exists(), \"alice_in_wonderland.md not found at DATA_PATH\"\n",
    "assert GOOGLE_API_KEY, \"Set GOOGLE_API_KEY in your environment\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "893ecd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Models ===\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-base-en-v1.5\"     # strong retriever (768d)\n",
    "RERANK_MODEL_NAME = \"BAAI/bge-reranker-base\"   # local cross-encoder reranker\n",
    "GEN_MODEL_NAME = \"gemini-1.5-flash\"            # your LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ca5b0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] Using:\n",
      " - Embeddings: BAAI/bge-base-en-v1.5\n",
      " - Reranker:   BAAI/bge-reranker-base\n",
      " - LLM:        gemini-1.5-flash\n",
      " - Data file:  alice_in_wonderland.md\n",
      " - Chroma dir: None\n"
     ]
    }
   ],
   "source": [
    "# === Chunking ===\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 500\n",
    "\n",
    "print(\"[SETUP] Using:\")\n",
    "print(f\" - Embeddings: {EMBED_MODEL_NAME}\")\n",
    "print(f\" - Reranker:   {RERANK_MODEL_NAME}\")\n",
    "print(f\" - LLM:        {GEN_MODEL_NAME}\")\n",
    "print(f\" - Data file:  {DATA_PATH}\")\n",
    "print(f\" - Chroma dir: {PERSIST_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "a0f8cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chapters(full_text: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parses Gutenberg-like CHAPTER headers. Matches e.g.:\n",
    "      CHAPTER I. Down the Rabbit-Hole\n",
    "      CHAPTER II The Pool of Tears\n",
    "    \"\"\"\n",
    "    chapters = []\n",
    "    pattern = r\"(?m)^(CHAPTER\\s+([IVXLCDM]+)\\.?\\s*(.*))\\s*$\"\n",
    "    for m in re.finditer(pattern, full_text):\n",
    "        chapters.append({\n",
    "            \"start\": m.start(),\n",
    "            \"end\": None,\n",
    "            \"chapter_roman\": (m.group(2) or \"\").strip(),\n",
    "            \"chapter_title\": (m.group(3) or \"\").strip(),\n",
    "            \"header\": m.group(1).strip()\n",
    "        })\n",
    "    for i in range(len(chapters)-1):\n",
    "        chapters[i][\"end\"] = chapters[i+1][\"start\"]\n",
    "    if chapters:\n",
    "        chapters[-1][\"end\"] = len(full_text)\n",
    "    return chapters\n",
    "\n",
    "def chapter_for_pos(chapters: List[Dict], pos: int) -> Dict:\n",
    "    if not chapters:\n",
    "        return {}\n",
    "    for ch in chapters:\n",
    "        if ch[\"start\"] <= pos < ch[\"end\"]:\n",
    "            return ch\n",
    "    return chapters[-1]\n",
    "\n",
    "def generate_chunk_summary(chunk_text: str, llm) -> str:\n",
    "    \"\"\"Generate a brief summary of the chunk content.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Generate a very brief (1-2 sentences) summary of the following text chunk from Alice in Wonderland.\n",
    "    Focus on the key events, characters, or dialogue.\n",
    "    \n",
    "    Text chunk:\n",
    "    {chunk_text}\n",
    "    \n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "def load_and_chunk_markdown(md_path: str, llm) -> List:\n",
    "    text = Path(md_path).read_text(encoding=\"utf-8\")\n",
    "    chapters = parse_chapters(text)\n",
    "    print(f\"[INGEST] Parsed {len(chapters)} chapter headers\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    docs = splitter.create_documents([text], metadatas=[{\"source\": os.path.basename(md_path)}])\n",
    "\n",
    "    total_chunks = len(docs)\n",
    "    print(f\"[INGEST] Generating summaries for {total_chunks} chunks...\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        s = d.metadata.get(\"start_index\", 0)\n",
    "        ch = chapter_for_pos(chapters, s)\n",
    "        \n",
    "        # Generate summary for the chunk\n",
    "        chunk_summary = generate_chunk_summary(d.page_content, llm)\n",
    "        \n",
    "        # Show progress every 10%\n",
    "        if i % max(1, total_chunks // 10) == 0:\n",
    "            print(f\"Progress: {i}/{total_chunks} chunks processed ({(i/total_chunks*100):.1f}%)\")\n",
    "        \n",
    "        d.metadata.update({\n",
    "            \"file\": os.path.basename(md_path),\n",
    "            \"start_index\": s,\n",
    "            \"chapter_number\": ch.get(\"chapter_roman\"),\n",
    "            \"chapter_title\": ch.get(\"chapter_title\"),\n",
    "            \"chapter_header\": ch.get(\"header\"),\n",
    "            \"chunk_summary\": chunk_summary\n",
    "        })\n",
    "    print(f\"[INGEST] Created {len(docs)} chunks with chapter-aware metadata and summaries\")\n",
    "    return docs\n",
    "\n",
    "# Documents will be created in build_vectorstore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2d8d5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vectorstore():\n",
    "    # Initialize LLM first\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=GEN_MODEL_NAME,\n",
    "        temperature=0.2,\n",
    "        convert_system_message_to_human=True,\n",
    "        google_api_key=GOOGLE_API_KEY,\n",
    "    )\n",
    "    \n",
    "    # Create documents with summaries\n",
    "    print(\"[INGEST] Creating documents with chunk summaries...\")\n",
    "    docs = load_and_chunk_markdown(DATA_PATH, llm)\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBED_MODEL_NAME,\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # cosine-ready\n",
    "        model_kwargs={\"token\": HF_TOKEN}  # Pass token to the model\n",
    "    )\n",
    "\n",
    "    print(\"[EMBED] Building Chroma collection with cosine space...\")\n",
    "    try:\n",
    "        # Create vectorstore in memory\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"alice\",\n",
    "            collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        return embeddings, vectordb, llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error building vectorstore: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Vectorstore will be built in the main execution cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "cb988c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_embeddings_info(embeddings, vectordb):\n",
    "    print(\"\\n[üîπ EMBEDDINGS INFO]\")\n",
    "    print(f\"Embedding model: {EMBED_MODEL_NAME}\")\n",
    "    dim = None\n",
    "    try:\n",
    "        dim = embeddings.client.get_sentence_embedding_dimension()\n",
    "    except Exception:\n",
    "        try:\n",
    "            sample = embeddings.embed_query(\"test\")\n",
    "            dim = len(sample)\n",
    "        except Exception:\n",
    "            dim = \"unknown\"\n",
    "    print(f\"Embedding dimension: {dim}\")\n",
    "    print(f\"Chroma collection: {vectordb._collection.name}\")\n",
    "    print(f\"Chroma vectors stored: {vectordb._collection.count()}\")\n",
    "    print(f\"Persist directory: {PERSIST_DIR}\")\n",
    "\n",
    "def log_multiquery_expansions(question: str, expansions: List[str]):\n",
    "    print(\"\\n[üîπ MULTI-QUERY EXPANSIONS]\")\n",
    "    print(f\"Original question: {question}\")\n",
    "    for i, q in enumerate(expansions, 1):\n",
    "        print(f\"  Q{i}: {q}\")\n",
    "\n",
    "def log_retrieved_candidates(candidates: List[Dict]):\n",
    "    # Group candidates by their matched query\n",
    "    query_groups = {}\n",
    "    for c in candidates:\n",
    "        q = c.get(\"matched_query\")\n",
    "        if q not in query_groups:\n",
    "            query_groups[q] = []\n",
    "        query_groups[q].append(c)\n",
    "    \n",
    "    print(\"\\n[üîπ RETRIEVED CANDIDATES BY QUERY]\")\n",
    "    \n",
    "    # First print original query results\n",
    "    original_query = list(query_groups.keys())[0]  # First query is the original\n",
    "    print(\"\\nORIGINAL QUESTION:\")\n",
    "    print(f\"'{original_query}'\")\n",
    "    print(\"\\nRETRIEVED CHUNKS:\")\n",
    "    for i, c in enumerate(query_groups[original_query], 1):\n",
    "        d = c[\"doc\"]\n",
    "        md = d.metadata\n",
    "        ann = c.get(\"ann_score\")\n",
    "        print(f\"\\nChunk {i} (Similarity Score: {ann:.4f})\")\n",
    "        print(f\"Location: Chapter {md.get('chapter_number')}: {md.get('chapter_title')}\")\n",
    "        print(f\"Summary: {md.get('chunk_summary', 'No summary available')}\")\n",
    "        print(f\"Content: {d.page_content[:200]}...\")\n",
    "        print(\"\\nMetadata:\")\n",
    "        for k, v in md.items():\n",
    "            if k not in ['chunk_summary', 'source']:\n",
    "                print(f\"- {k}: {v}\")\n",
    "    \n",
    "    # Then print expansion query results\n",
    "    for query in list(query_groups.keys())[1:]:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"\\nGENERATED QUESTION:\")\n",
    "        print(f\"'{query}'\")\n",
    "        print(\"\\nRETRIEVED CHUNKS:\")\n",
    "        for i, c in enumerate(query_groups[query], 1):\n",
    "            d = c[\"doc\"]\n",
    "            md = d.metadata\n",
    "            ann = c.get(\"ann_score\")\n",
    "            print(f\"\\nChunk {i} (Similarity Score: {ann:.4f})\")\n",
    "            print(f\"Location: Chapter {md.get('chapter_number')}: {md.get('chapter_title')}\")\n",
    "            print(f\"Summary: {md.get('chunk_summary', 'No summary available')}\")\n",
    "            print(f\"Content: {d.page_content[:200]}...\")\n",
    "            print(\"\\nMetadata:\")\n",
    "            for k, v in md.items():\n",
    "                if k not in ['chunk_summary', 'source']:\n",
    "                    print(f\"- {k}: {v}\")\n",
    "\n",
    "def log_reranked(top_docs: List, top_scores: List, enriched_all: List[Dict]):\n",
    "    print(\"\\n[üîπ RERANKED RESULTS (cross-encoder)]\")\n",
    "    for i, (d, s) in enumerate(zip(top_docs, top_scores), 1):\n",
    "        md = d.metadata\n",
    "        # find the candidate to also print its pre-rerank ANN score\n",
    "        ann = None\n",
    "        for item in enriched_all:\n",
    "            if item[\"doc\"] is d and \"ann_score\" in item:\n",
    "                ann = item[\"ann_score\"]\n",
    "                break\n",
    "        preview = d.page_content[:140].replace(\"\\n\", \" \") + \"...\"\n",
    "        print(f\"{i:>2}. RERANK={s:.4f} | ANN={ann:.4f} | {md.get('file')} | Ch.{md.get('chapter_number')} | start={md.get('start_index')}\")\n",
    "        print(f\"    Title: {md.get('chapter_title')}\")\n",
    "        print(f\"    Preview: {preview}\\n\")\n",
    "\n",
    "def log_final_answer(question: str, answer: str, docs: List):\n",
    "    print(\"\\n[üîπ FINAL ANSWER]\")\n",
    "    print(f\"Question:\\n{question}\\n\")\n",
    "    print(\"Answer:\\n\" + answer + \"\\n\")\n",
    "    print(\"Sources used:\")\n",
    "    for d in docs:\n",
    "        md = d.metadata\n",
    "        print(f\" - {md.get('file')} | Chapter {md.get('chapter_number')}: {md.get('chapter_title')} | Position: {md.get('start_index')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3a099027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM will be initialized in build_vectorstore()\n",
    "\n",
    "def _extract_json_array(text: str) -> str:\n",
    "    m = re.search(r\"\\[.*\\]\", text, re.S)\n",
    "    return m.group(0) if m else \"[]\"\n",
    "\n",
    "def expand_queries(llm, question: str, n: int = 4) -> List[str]:\n",
    "    prompt = f\"\"\"\n",
    "You are helping expand a user's query about the novel \"Alice's Adventures in Wonderland\".\n",
    "Generate {n} diverse, *meaningfully different* paraphrases that could retrieve relevant passages.\n",
    "Return ONLY a JSON array of strings.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "    resp = llm.invoke(prompt)\n",
    "    out = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "    try:\n",
    "        expansions = json.loads(_extract_json_array(out))\n",
    "    except Exception:\n",
    "        expansions = [ln.strip(\"-‚Ä¢ \").strip() for ln in out.splitlines() if ln.strip()]\n",
    "    # remove empties and exact duplicates of original\n",
    "    expansions = [e for e in expansions if e and e.strip() and e.strip() != question.strip()]\n",
    "    return expansions\n",
    "\n",
    "def retrieve_candidates(vectordb: Chroma, queries: List[str], per_query_k: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    For each query, run ANN retrieval with scores.\n",
    "    Returns list of dicts: {\"doc\": Document, \"ann_score\": float, \"matched_query\": str}\n",
    "    Maintains separate results for each query while avoiding exact duplicates.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    seen_keys = set()  # Track unique documents\n",
    "    \n",
    "    # First add results from original query\n",
    "    original_query = queries[0]\n",
    "    results = vectordb.similarity_search_with_relevance_scores(original_query, k=per_query_k)\n",
    "    for doc, score in results:\n",
    "        key = (doc.metadata.get(\"file\"), doc.metadata.get(\"start_index\"))\n",
    "        if key not in seen_keys:\n",
    "            seen_keys.add(key)\n",
    "            all_results.append({\n",
    "                \"doc\": doc,\n",
    "                \"ann_score\": float(score),\n",
    "                \"matched_query\": original_query\n",
    "            })\n",
    "    \n",
    "    # Then add unique results from expansion queries\n",
    "    for q in queries[1:]:  # Skip the original query\n",
    "        results = vectordb.similarity_search_with_relevance_scores(q, k=per_query_k)\n",
    "        for doc, score in results:\n",
    "            key = (doc.metadata.get(\"file\"), doc.metadata.get(\"start_index\"))\n",
    "            if key not in seen_keys:\n",
    "                seen_keys.add(key)\n",
    "                all_results.append({\n",
    "                    \"doc\": doc,\n",
    "                    \"ann_score\": float(score),\n",
    "                    \"matched_query\": q\n",
    "                })\n",
    "    \n",
    "    # Sort all results by score while maintaining query grouping\n",
    "    all_results.sort(key=lambda x: (x[\"matched_query\"] != original_query, -x[\"ann_score\"]))\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "18ce8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = FlagReranker(RERANK_MODEL_NAME, use_fp16=True)\n",
    "\n",
    "def rerank_candidates(question: str, candidates: List[Dict], top_n: int = 6) -> Tuple[List, List[float], List[Dict]]:\n",
    "    if not candidates:\n",
    "        return [], [], []\n",
    "    docs = [c[\"doc\"] for c in candidates]\n",
    "    pairs = [[question, d.page_content] for d in docs]\n",
    "    scores = reranker.compute_score(pairs)  # list of floats, higher is better\n",
    "    enriched = []\n",
    "    for c, s in zip(candidates, scores):\n",
    "        e = dict(c)\n",
    "        e[\"rerank_score\"] = float(s)\n",
    "        enriched.append(e)\n",
    "    enriched.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    top = enriched[:top_n]\n",
    "    top_docs = [t[\"doc\"] for t in top]\n",
    "    top_scores = [t[\"rerank_score\"] for t in top]\n",
    "    return top_docs, top_scores, enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "ae375542",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "You are a careful literary assistant answering questions about *Alice's Adventures in Wonderland*.\n",
    "Use ONLY the provided context. If the context is insufficient, say so briefly and ask for a clarification.\n",
    "\n",
    "Return:\n",
    "1) A precise answer.\n",
    "2) (Optional) 1‚Äì3 brief quotes (‚â§ 2 sentences each).\n",
    "3) Sources as: [file]:[chapter]:[start]\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "def join_with_citations(docs: List) -> str:\n",
    "    parts = []\n",
    "    for d in docs:\n",
    "        md = d.metadata\n",
    "        header = f\"[Source: {md.get('file')} | Chapter {md.get('chapter_number')}: {md.get('chapter_title')} | Position: {md.get('start_index')}]\"\n",
    "        parts.append(header + \"\\n\" + d.page_content.strip())\n",
    "    return \"\\n\\n---\\n\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "4be20dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INGEST] Creating documents with chunk summaries...\n",
      "[INGEST] Parsed 24 chapter headers\n",
      "[INGEST] Generating summaries for 258 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[395], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build the vectorstore and get components\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m embeddings, vectordb, llm \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manswer_query\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, per_query_k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, expansions_n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m, final_k: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=========================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[390], line 12\u001b[0m, in \u001b[0;36mbuild_vectorstore\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create documents with summaries\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INGEST] Creating documents with chunk summaries...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_chunk_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize embeddings\u001b[39;00m\n\u001b[1;32m     15\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(\n\u001b[1;32m     16\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mEMBED_MODEL_NAME,\n\u001b[1;32m     17\u001b[0m     encode_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},  \u001b[38;5;66;03m# cosine-ready\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m: HF_TOKEN}  \u001b[38;5;66;03m# Pass token to the model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[389], line 65\u001b[0m, in \u001b[0;36mload_and_chunk_markdown\u001b[0;34m(md_path, llm)\u001b[0m\n\u001b[1;32m     62\u001b[0m ch \u001b[38;5;241m=\u001b[39m chapter_for_pos(chapters, s)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Generate summary for the chunk\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m chunk_summary \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_chunk_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Show progress every 10%\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_chunks \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[389], line 42\u001b[0m, in \u001b[0;36mgenerate_chunk_summary\u001b[0;34m(chunk_text, llm)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a brief summary of the chunk content.\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124mGenerate a very brief (1-2 sentences) summary of the following text chunk from Alice in Wonderland.\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124mFocus on the key events, characters, or dialogue.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124m\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124mSummary:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(response)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:1488\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI.invoke\u001b[0;34m(self, input, config, code_execution, stop, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1485\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools are already defined.code_execution tool can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be defined\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1486\u001b[0m         )\n\u001b[0;32m-> 1488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:383\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    379\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    380\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 383\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    393\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1006\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1004\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1005\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:825\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 825\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m         )\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1072\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1072\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1076\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:1595\u001b[0m, in \u001b[0;36mChatGoogleGenerativeAI._generate\u001b[0;34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1570\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m   1583\u001b[0m     request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[1;32m   1584\u001b[0m         messages,\n\u001b[1;32m   1585\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1594\u001b[0m     )\n\u001b[0;32m-> 1595\u001b[0m     response: GenerateContentResponse \u001b[38;5;241m=\u001b[39m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:247\u001b[0m, in \u001b[0;36m_chat_with_retry\u001b[0;34m(generation_method, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    240\u001b[0m params \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    241\u001b[0m     {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service}\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (request \u001b[38;5;241m:=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m kwargs\n\u001b[1;32m    246\u001b[0m )\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tenacity/__init__.py:485\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    484\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build the vectorstore and get components\n",
    "embeddings, vectordb, llm = build_vectorstore()\n",
    "\n",
    "def answer_query(question: str, per_query_k: int = 10, expansions_n: int = 4, final_k: int = 6):\n",
    "    print(\"\\n=========================\")\n",
    "    print(f\"üîç Processing question: {question}\")\n",
    "    print(\"=========================\")\n",
    "\n",
    "    # 0) Embeddings/DB summary\n",
    "    log_embeddings_info(embeddings, vectordb)\n",
    "\n",
    "    # 1) Multi-Query expansions\n",
    "    expansions = expand_queries(llm, question, n=expansions_n)\n",
    "    log_multiquery_expansions(question, expansions)\n",
    "    queries = [question] + expansions\n",
    "\n",
    "    # 2) Retrieval across (original + expansions)\n",
    "    candidates = retrieve_candidates(vectordb, queries, per_query_k=per_query_k)\n",
    "    log_retrieved_candidates(candidates)\n",
    "\n",
    "    # 3) Rerank with local cross-encoder\n",
    "    top_docs, top_scores, enriched_all = rerank_candidates(question, candidates, top_n=final_k)\n",
    "    log_reranked(top_docs, top_scores, enriched_all)\n",
    "\n",
    "    # 4) Build context & ask LLM\n",
    "    context = join_with_citations(top_docs)\n",
    "    prompt = RAG_PROMPT.format(context=context, question=question)\n",
    "    resp = llm.invoke(prompt)\n",
    "    answer_text = resp.content if hasattr(resp, \"content\") else str(resp)\n",
    "\n",
    "    # 5) Log final result\n",
    "    log_final_answer(question, answer_text, top_docs)\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer_text,\n",
    "        \"retrieved_count\": len(candidates),\n",
    "        \"used_count\": len(top_docs),\n",
    "        \"sources\": [\n",
    "            {\n",
    "                \"file\": d.metadata.get(\"file\"),\n",
    "                \"chapter_number\": d.metadata.get(\"chapter_number\"),\n",
    "                \"chapter_title\": d.metadata.get(\"chapter_title\"),\n",
    "                \"start_index\": d.metadata.get(\"start_index\"),\n",
    "                \"chunk_summary\": d.metadata.get(\"chunk_summary\", \"No summary available\")\n",
    "            }\n",
    "            for d in top_docs\n",
    "        ],\n",
    "        \"expansions\": expansions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68db00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "üîç Processing question: Why did Alice follow the White Rabbit and what happened immediately after?\n",
      "=========================\n",
      "\n",
      "[üîπ EMBEDDINGS INFO]\n",
      "Embedding model: BAAI/bge-base-en-v1.5\n",
      "Embedding dimension: 768\n",
      "Chroma collection: alice\n",
      "Chroma vectors stored: 723\n",
      "Persist directory: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[üîπ MULTI-QUERY EXPANSIONS]\n",
      "Original question: Why did Alice follow the White Rabbit and what happened immediately after?\n",
      "  Q1: What compelled Alice to chase the White Rabbit, and what was her experience falling down the rabbit hole?\n",
      "  Q2: Describe Alice's pursuit of the White Rabbit and the beginning of her journey into Wonderland.\n",
      "  Q3: Why did Alice go down the rabbit hole, and what did she encounter during her descent?\n",
      "  Q4: What made Alice follow the White Rabbit, and what was the immediate environment like after she entered the hole?\n",
      "\n",
      "[üîπ RETRIEVED CANDIDATES BY QUERY (pre-rerank)]\n",
      "\n",
      "Results for query: Why did Alice follow the White Rabbit and what happened immediately after?\n",
      "--------------------------------------------------------------------------------\n",
      " 1. ANN=0.7410 | Ch.XII: Alice‚Äôs Evidence\n",
      "    Preview: ‚ÄúThat _proves_ his guilt,‚Äù said the Queen.  ‚ÄúIt proves nothing of the sort!‚Äù said Alice. ‚ÄúWhy, you don‚Äôt even know what they‚Äôre about!‚Äù  ‚ÄúRe...\n",
      "\n",
      " 2. ANN=0.7232 | Ch.I: Down the Rabbit-Hole\n",
      "    Preview: There was nothing so _very_ remarkable in that; nor did Alice think it so _very_ much out of the way to hear the Rabbit say to itself, ‚ÄúOh d...\n",
      "\n",
      " 3. ANN=0.7212 | Ch.XI: Who Stole the Tarts?\n",
      "    Preview: ‚ÄúNever mind!‚Äù said the King, with an air of great relief. ‚ÄúCall the next witness.‚Äù And he added in an undertone to the Queen, ‚ÄúReally, my de...\n",
      "\n",
      " 4. ANN=0.7204 | Ch.II: The Pool of Tears\n",
      "    Preview: After a time she heard a little pattering of feet in the distance, and she hastily dried her eyes to see what was coming. It was the White R...\n",
      "\n",
      "\n",
      "Results for query: What compelled Alice to chase the White Rabbit, and what was her experience falling down the rabbit hole?\n",
      "--------------------------------------------------------------------------------\n",
      " 1. ANN=0.7422 | Ch.I: Down the Rabbit-Hole\n",
      "    Preview: CHAPTER I. Down the Rabbit-Hole  Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: on...\n",
      "\n",
      " 2. ANN=0.7411 | Ch.I: Down the Rabbit-Hole\n",
      "    Preview: In another moment down went Alice after it, never once considering how in the world she was to get out again.  The rabbit-hole went straight...\n",
      "\n",
      " 3. ANN=0.6807 | Ch.I: Down the Rabbit-Hole\n",
      "    Preview: Alice was not a bit hurt, and she jumped up on to her feet in a moment: she looked up, but it was all dark overhead; before her was another ...\n",
      "\n",
      "\n",
      "Results for query: Describe Alice's pursuit of the White Rabbit and the beginning of her journey into Wonderland.\n",
      "--------------------------------------------------------------------------------\n",
      " 1. ANN=0.7167 | Ch.X: The Lobster Quadrille\n",
      "    Preview: ‚ÄúNo, no! The adventures first,‚Äù said the Gryphon in an impatient tone: ‚Äúexplanations take such a dreadful time.‚Äù  So Alice began telling the...\n",
      "\n",
      " 2. ANN=0.7031 | Ch.VIII: The Queen‚Äôs Croquet-Ground\n",
      "    Preview: First came ten soldiers carrying clubs; these were all shaped like the three gardeners, oblong and flat, with their hands and feet at the co...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[üîπ RERANKED RESULTS (cross-encoder)]\n",
      " 1. RERANK=0.1173 | ANN=0.7167 | alice_in_wonderland.md | Ch.X | start=117915\n",
      "    Title: The Lobster Quadrille\n",
      "    Preview: ‚ÄúNo, no! The adventures first,‚Äù said the Gryphon in an impatient tone: ‚Äúexplanations take such a dreadful time.‚Äù  So Alice began telling the...\n",
      "\n",
      " 2. RERANK=-0.9883 | ANN=0.7204 | alice_in_wonderland.md | Ch.II | start=14431\n",
      "    Title: The Pool of Tears\n",
      "    Preview: After a time she heard a little pattering of feet in the distance, and she hastily dried her eyes to see what was coming. It was the White R...\n",
      "\n",
      " 3. RERANK=-1.2969 | ANN=0.7031 | alice_in_wonderland.md | Ch.VIII | start=86935\n",
      "    Title: The Queen‚Äôs Croquet-Ground\n",
      "    Preview: First came ten soldiers carrying clubs; these were all shaped like the three gardeners, oblong and flat, with their hands and feet at the co...\n",
      "\n",
      " 4. RERANK=-1.5840 | ANN=0.7422 | alice_in_wonderland.md | Ch.I | start=1309\n",
      "    Title: Down the Rabbit-Hole\n",
      "    Preview: CHAPTER I. Down the Rabbit-Hole  Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: on...\n",
      "\n",
      " 5. RERANK=-1.8428 | ANN=0.6807 | alice_in_wonderland.md | Ch.I | start=6364\n",
      "    Title: Down the Rabbit-Hole\n",
      "    Preview: Alice was not a bit hurt, and she jumped up on to her feet in a moment: she looked up, but it was all dark overhead; before her was another ...\n",
      "\n",
      " 6. RERANK=-2.3262 | ANN=0.7411 | alice_in_wonderland.md | Ch.I | start=2678\n",
      "    Title: Down the Rabbit-Hole\n",
      "    Preview: In another moment down went Alice after it, never once considering how in the world she was to get out again.  The rabbit-hole went straight...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:499: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[üîπ FINAL ANSWER]\n",
      "Question:\n",
      "Why did Alice follow the White Rabbit and what happened immediately after?\n",
      "\n",
      "Answer:\n",
      "Alice followed the White Rabbit impulsively after seeing it run close by and then hurry down a passage, feeling there was \"not a moment to be lost.\" Immediately after she went down the rabbit-hole, she found herself falling down a very deep well because the hole dipped suddenly.\n",
      "\n",
      "* \"when suddenly a White Rabbit with pink eyes ran close by her.\"\n",
      "* \"There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ‚ÄúOh my ears and whiskers, how late it‚Äôs getting!‚Äù\"\n",
      "* \"The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\"\n",
      "\n",
      "[alice_in_wonderland.md]:Chapter I: Down the Rabbit-Hole:1309\n",
      "[alice_in_wonderland.md]:Chapter I: Down the Rabbit-Hole:6364\n",
      "[alice_in_wonderland.md]:Chapter I: Down the Rabbit-Hole:2678\n",
      "\n",
      "Sources used:\n",
      " - alice_in_wonderland.md | Chapter X: The Lobster Quadrille | Position: 117915\n",
      " - alice_in_wonderland.md | Chapter II: The Pool of Tears | Position: 14431\n",
      " - alice_in_wonderland.md | Chapter VIII: The Queen‚Äôs Croquet-Ground | Position: 86935\n",
      " - alice_in_wonderland.md | Chapter I: Down the Rabbit-Hole | Position: 1309\n",
      " - alice_in_wonderland.md | Chapter I: Down the Rabbit-Hole | Position: 6364\n",
      " - alice_in_wonderland.md | Chapter I: Down the Rabbit-Hole | Position: 2678\n"
     ]
    }
   ],
   "source": [
    "res = answer_query(\n",
    "    \"Why did Alice follow the White Rabbit and what happened immediately after?\",\n",
    "    per_query_k=10,    # candidates per (sub)query\n",
    "    expansions_n=4,    # number of multi-query expansions\n",
    "    final_k=6          # final top-N after rerank sent to LLM\n",
    ")\n",
    "\n",
    "# 'res' also contains expansions, counts, and structured sources if you need them programmatically\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
