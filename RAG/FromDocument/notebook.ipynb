{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407e2cf0",
   "metadata": {},
   "source": [
    "Here's the diagram for the process : [rag_image](mermaid_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9911ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Install dependencies\n",
    "\n",
    "%pip install langchain langchain-community langchain-huggingface langchain-google-genai --quiet\n",
    "%pip install chromadb sentence-transformers FlagEmbedding huggingface_hub --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7225c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Imports\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "\n",
    "# LangChain components\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Reranker\n",
    "from FlagEmbedding import FlagReranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8600151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Authentication (Hugging Face + Gemini key)\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Hugging Face login\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Gemini API key\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "assert GOOGLE_API_KEY, \"Gemini API Key not found!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a4e1f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Summarizing chunks with OpenRouter gpt-oss-20b (token-aware splitting)...\n",
      "### Example 3 Chunks ###\n",
      "{\n",
      "  \"content\": \"The Project Gutenberg eBook of Alice's Adventures in Wonderland\\n\\nThis ebook is for the use of anyone anywhere in the United States and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\nof the Project Gutenberg License included with this ebook or online\\nat www.gutenberg.org. If you are not located in the United States,\\nyou will have to check the laws of the country where you are located\\nbefore using this eBook.\\n\\nTitle: Alice's Adventures in Wonderland\\n\\nAuthor: Lewis Carroll\\n\\nRelease date: June 27, 2008 [eBook #11]\\nMost recently updated: March 30, 2021\\n\\nLanguage: English\\n\\nCredits: Arthur DiBianca and David Widger\\n\\n**_ START OF THE PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND _**\\n\\nAlice\\u2019s Adventures in Wonderland\\n\\nby Lewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\nContents\\n\\nCHAPTER I. Down the Rabbit-Hole\\nCHAPTER II. The Pool of Tears\\nCHAPTER III. A Caucus-Race and a Long Tale\\nCHAPTER IV. The Rabbit Sends in a Little Bill\\nCHAPTER V. Advice from a Caterpillar\\nCHAPTER VI. Pig and Pepper\\nCHAPTER VII. A Mad Tea-Party\\nCHAPTER VIII. The Queen\\u2019s Croquet-Ground\\nCHAPTER IX. The Mock Turtle\\u2019s Story\\nCHAPTER X. The Lobster Quadrille\\nCHAPTER XI. Who Stole the Tarts?\\nCHAPTER XII. Alice\\u2019s Evidence\\n\\nCHAPTER I.\\nDown the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, \\u201cand what is the use of a book,\\u201d thought Alice\\n\\u201cwithout pictures or conversations?\\u201d\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\",\n",
      "  \"metadata\": {\n",
      "    \"source\": \"alice_in_wonderland.md\",\n",
      "    \"chapter_number\": null,\n",
      "    \"chapter_title\": null,\n",
      "    \"start_index\": 0,\n",
      "    \"chunk_number\": 0,\n",
      "    \"chunk_summary\": \"The Project Gutenberg eBook of Lewis Carroll\\u2019s *Alice\\u2019s Adventures in Wonderland* is freely available worldwide, with the text beginning in Chapter\\u202fI, \\u201cDown the Rabbit\\u2011Hole.\\u201d  In the opening, Alice is \\u201cbeginning to get very tired of sitting by her sister on the bank\\u201d and muses that a book \\u201cwithout pictures or conversations\\u201d is useless, before a \\u201cWhite Rabbit with pink eyes\\u201d runs past her.  The passage sets up Alice\\u2019s boredom and the inciting encounter that launches her fantastical journey.\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"content\": \"So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so _very_ remarkable in that; nor did Alice think it\\nso _very_ much out of the way to hear the Rabbit say to itself, \\u201cOh\\ndear! Oh dear! I shall be late!\\u201d (when she thought it over afterwards,\\nit occurred to her that she ought to have wondered at this, but at the\\ntime it all seemed quite natural); but when the Rabbit actually _took a\\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\\non, Alice started to her feet, for it flashed across her mind that she\\nhad never before seen a rabbit with either a waistcoat-pocket, or a\\nwatch to take out of it, and burning with curiosity, she ran across the\\nfield after it, and fortunately was just in time to see it pop down a\\nlarge rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herself falling down a very\\ndeep well.\\n\\nEither the well was very deep, or she fell very slowly, for she had\\nplenty of time as she went down to look about her and to wonder what\\nwas going to happen next. First, she tried to look down and make out\\nwhat she was coming to, but it was too dark to see anything; then she\\nlooked at the sides of the well, and noticed that they were filled with\\ncupboards and book-shelves; here and there she saw maps and pictures\\nhung upon pegs. She took down a jar from one of the shelves as she\\npassed; it was labelled \\u201cORANGE MARMALADE\\u201d, but to her great\\ndisappointment it was empty: she did not like to drop the jar for fear\\nof killing somebody underneath, so managed to put it into one of the\\ncupboards as she fell past it.\",\n",
      "  \"metadata\": {\n",
      "    \"source\": \"alice_in_wonderland.md\",\n",
      "    \"chapter_number\": null,\n",
      "    \"chapter_title\": null,\n",
      "    \"start_index\": -1,\n",
      "    \"chunk_number\": 1,\n",
      "    \"chunk_summary\": \"Alice, feeling sleepy on a hot day, muses whether the pleasure of making a daisy\\u2011chain is worth the trouble of picking daisies when a White Rabbit with pink eyes rushes past her, exclaiming \\u201cOh dear! Oh dear! I shall be late!\\u201d and pulling a watch from its waistcoat\\u2011pocket\\u2014an odd sight that sparks her curiosity and leads her to chase the rabbit down a large rabbit\\u2011hole. She falls into a deep well, where she notices cupboards and bookshelves lining the walls, and even picks up a jar labeled \\u201cORANGE MARMALADE,\\u201d though it is empty, and she carefully places it back to avoid disturbing anyone below.\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"content\": \"\\u201cWell!\\u201d thought Alice to herself, \\u201cafter such a fall as this, I shall\\nthink nothing of tumbling down stairs! How brave they\\u2019ll all think me\\nat home! Why, I wouldn\\u2019t say anything about it, even if I fell off the\\ntop of the house!\\u201d (Which was very likely true.)\\n\\nDown, down, down. Would the fall _never_ come to an end? \\u201cI wonder how\\nmany miles I\\u2019ve fallen by this time?\\u201d she said aloud. \\u201cI must be\\ngetting somewhere near the centre of the earth. Let me see: that would\\nbe four thousand miles down, I think\\u2014\\u201d (for, you see, Alice had learnt\\nseveral things of this sort in her lessons in the schoolroom, and\\nthough this was not a _very_ good opportunity for showing off her\\nknowledge, as there was no one to listen to her, still it was good\\npractice to say it over) \\u201c\\u2014yes, that\\u2019s about the right distance\\u2014but\\nthen I wonder what Latitude or Longitude I\\u2019ve got to?\\u201d (Alice had no\\nidea what Latitude was, or Longitude either, but thought they were nice\\ngrand words to say.)\\n\\nPresently she began again. \\u201cI wonder if I shall fall right _through_\\nthe earth! How funny it\\u2019ll seem to come out among the people that walk\\nwith their heads downward! The Antipathies, I think\\u2014\\u201d (she was rather\\nglad there _was_ no one listening, this time, as it didn\\u2019t sound at all\\nthe right word) \\u201c\\u2014but I shall have to ask them what the name of the\\ncountry is, you know. Please, Ma\\u2019am, is this New Zealand or Australia?\\u201d\\n(and she tried to curtsey as she spoke\\u2014fancy _curtseying_ as you\\u2019re\\nfalling through the air! Do you think you could manage it?) \\u201cAnd what\\nan ignorant little girl she\\u2019ll think me for asking! No, it\\u2019ll never do\\nto ask: perhaps I shall see it written up somewhere.\\u201d\",\n",
      "  \"metadata\": {\n",
      "    \"source\": \"alice_in_wonderland.md\",\n",
      "    \"chapter_number\": \"Chapter I\",\n",
      "    \"chapter_title\": \"Down the Rabbit-Hole\",\n",
      "    \"start_index\": 3727,\n",
      "    \"chunk_number\": 2,\n",
      "    \"chunk_summary\": \"Alice, tumbling endlessly \\u201cDown, down, down,\\u201d muses that after such a fall she will \\u201cthink nothing of tumbling down stairs!\\u201d and even \\u201cwouldn't say anything about it, even if I fell off the top of the house!\\u201d (Which was very likely true.) She speculates aloud, \\u201cI wonder how many miles I\\u2019ve fallen by this time?\\u201d and calculates she\\u2019s about \\u201cfour thousand miles down, I think\\u2014yes, that\\u2019s about the right distance\\u201d\\u2014while also pondering \\u201cwhat Latitude or Longitude I\\u2019ve got to?\\u201d (Alice had no idea what Latitude was, or Longitude either, but thought they were nice grand words to say.) Finally, she imagines emerging \\u201cright through the earth\\u201d and encountering the \\u201cAntipathies,\\u201d asking \\u201cPlease, Ma\\u2019am, is this New Zealand or Australia?\\u201d as she tries to curtsey while falling.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Load and chunk the book (token-aware) + batch summarization via OpenRouter (gpt-oss-20b)\n",
    "import os, json, re, requests\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Summarization pipeline (OpenRouter: openai/gpt-oss-20b) ---\n",
    "class OpenRouterSummarizer:\n",
    "    def __init__(self, model=\"openai/gpt-oss-20b\", api_key=None):\n",
    "        self.model = model\n",
    "        self.api_key = api_key or os.getenv(\"OPENROUTER_API_KEY\")\n",
    "        assert self.api_key, \"OpenRouter API key not found. Set OPENROUTER_API_KEY in your .env.\"\n",
    "\n",
    "    # Keep the same call signature you used before\n",
    "    def __call__(self, texts, max_length=40, min_length=10, do_sample=False, batch_size=8):\n",
    "        url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "        results = []\n",
    "        for t in texts:\n",
    "            prompt = (\n",
    "                \"Summarize the passage in 2-3 sentences as per the content, capture the main point of the content and make sure you're not missing out on the quotes from the content.\"\n",
    "                f\"Passage:\\n{t}\\n\\nSummary:\"\n",
    "            )\n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": 0.2,\n",
    "            }\n",
    "            r = requests.post(url, headers=headers, json=payload, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            content = r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            results.append({\"summary_text\": content})\n",
    "        return results\n",
    "\n",
    "# Initialize summarizer (variable name unchanged)\n",
    "summarizer = OpenRouterSummarizer(model=\"openai/gpt-oss-20b\")\n",
    "\n",
    "# --- Function: parse chapters (captures both same-line and next-line titles) ---\n",
    "def parse_chapters(text):\n",
    "    \"\"\"\n",
    "    Matches:\n",
    "      Contents lines:  'CHAPTER I. Down the Rabbit-Hole'\n",
    "      Chapter headers: 'CHAPTER I.\\nDown the Rabbit-Hole'\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r\"CHAPTER\\s+([IVXLCDM]+)\\.\\s*(?:([^\\n]+)|\\n([^\\n]+))\")\n",
    "    chapters = []\n",
    "    for m in pattern.finditer(text):\n",
    "        title = (m.group(2) or m.group(3) or \"\").strip()\n",
    "        chapters.append({\n",
    "            \"start\": m.start(),\n",
    "            \"chapter_number\": f\"Chapter {m.group(1)}\",\n",
    "            \"chapter_title\": title\n",
    "        })\n",
    "    chapters.sort(key=lambda x: x[\"start\"])\n",
    "    return chapters\n",
    "\n",
    "# --- Function: split and summarize (token-aware, reliable start_index) ---\n",
    "def load_and_chunk_markdown(md_path, chunk_size=600, chunk_overlap=100, batch_size=10):\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_text = f.read()\n",
    "\n",
    "    chapters = parse_chapters(full_text)\n",
    "\n",
    "    # Token-aware splitter with start indices:\n",
    "    # NOTE: pass add_start_index=True on the splitter (NOT on create_documents)\n",
    "    try:\n",
    "        splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            add_start_index=True\n",
    "        )\n",
    "    except TypeError:\n",
    "        # Fallback for older LangChain versions: construct directly\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            add_start_index=True\n",
    "        )\n",
    "\n",
    "    # create_documents returns documents with metadata['start_index']\n",
    "    doc_objs = splitter.create_documents([full_text])\n",
    "\n",
    "    # Prepare raw chunk strings for summarizer call\n",
    "    chunks = [d.page_content for d in doc_objs]\n",
    "\n",
    "    # 🔹 Summarize chunks (OpenRouter; sequential for clarity)\n",
    "    summaries = summarizer(\n",
    "        chunks,\n",
    "        max_length=40,\n",
    "        min_length=10,\n",
    "        do_sample=False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    docs = []\n",
    "    for i, (d, summary_obj) in enumerate(zip(doc_objs, summaries)):\n",
    "        start_index = d.metadata.get(\"start_index\", None)\n",
    "        chunk = d.page_content\n",
    "        summary = summary_obj[\"summary_text\"]\n",
    "\n",
    "        # Find chapter for this chunk by position (binary search style)\n",
    "        chapter_number, chapter_title = None, None\n",
    "        if start_index is not None and chapters:\n",
    "            lo, hi, idx = 0, len(chapters)-1, -1\n",
    "            while lo <= hi:\n",
    "                mid = (lo + hi) // 2\n",
    "                if chapters[mid][\"start\"] <= start_index:\n",
    "                    idx = mid\n",
    "                    lo = mid + 1\n",
    "                else:\n",
    "                    hi = mid - 1\n",
    "            if idx >= 0:\n",
    "                chapter_number = chapters[idx][\"chapter_number\"]\n",
    "                chapter_title = chapters[idx][\"chapter_title\"]\n",
    "\n",
    "        docs.append({\n",
    "            \"content\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"source\": md_path,\n",
    "                \"chapter_number\": chapter_number,     # e.g., \"Chapter I\"\n",
    "                \"chapter_title\": chapter_title,       # e.g., \"Down the Rabbit-Hole\"\n",
    "                \"start_index\": start_index,\n",
    "                \"chunk_number\": i,\n",
    "                \"chunk_summary\": summary\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return docs\n",
    "\n",
    "# Load and chunk with batching\n",
    "print(\"⚡ Summarizing chunks with OpenRouter gpt-oss-20b (token-aware splitting)...\")\n",
    "docs = load_and_chunk_markdown(\"alice_in_wonderland.md\", batch_size=8)\n",
    "\n",
    "# #printing\n",
    "print(\"### Example 3 Chunks ###\")\n",
    "for d in docs[:3]:\n",
    "    print(json.dumps(d, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01228159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from_texts() takes each chunk from texts=[d[\"content\"] for d in docs].\\nFor each chunk, it calls embeddings.embed_text(text) under the hood.\\nembed_text converts the chunk into a high-dimensional vector (embedding).\\nThese vectors are then stored in Chroma along with your metadata. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from_texts() takes each chunk from texts=[d[\"content\"] for d in docs].\n",
    "For each chunk, it calls embeddings.embed_text(text) under the hood.\n",
    "embed_text converts the chunk into a high-dimensional vector (embedding).\n",
    "These vectors are then stored in Chroma along with your metadata. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac4d832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Vector DB Info ###\n",
      "Persist dir: ./chroma_store\n",
      "Total vectors stored: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/52/ns3sdnhj22n2q3hsq7mpbgv40000gn/T/ipykernel_4188/139240837.py:13: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()  # save to disk\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Build embeddings + vectorstore (persistent Chroma; cosine)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=[d[\"content\"] for d in docs],\n",
    "    embedding=embeddings,\n",
    "    metadatas=[d[\"metadata\"] for d in docs],\n",
    "    persist_directory=\"./chroma_store\",              \n",
    "    collection_name=\"alice\",\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}     \n",
    ")\n",
    "vectordb.persist()  # save to disk\n",
    "\n",
    "# #printing\n",
    "print(\"### Vector DB Info ###\")\n",
    "print(\"Persist dir:\", \"./chroma_store\")\n",
    "print(\"Total vectors stored:\", vectordb._collection.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f81f12ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Multi-Query Expansions ###\n",
      "Original: Why did Alice follow the White Rabbit and what happened immediately after she fell down the rabbit hole?\n",
      "Q1: What led Alice to go after the White Rabbit, and what events unfolded right after her descent into the rabbit hole?\n",
      "Q2: Could you explain why Alice followed the White Rabbit, and describe her initial experience directly upon falling into the rabbit hole?\n",
      "Q3: What was Alice's motivation for pursuing the White Rabbit, and what transpired immediately after her fall down the rabbit hole?\n",
      "Q4: What prompted Alice to chase the White Rabbit, and what were the first things that occurred after she tumbled down the rabbit hole?\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Multi-query expansions (use Gemini)\n",
    "# Generate 4 alternative phrasings of the user question\n",
    "\n",
    "def expand_queries(llm, question, n=4):\n",
    "    prompt = f\"\"\"\n",
    "    Generate {n} different phrasings of the following user question:\n",
    "    \"{question}\"\n",
    "    Provide only the variations, one per line.\n",
    "    \"\"\"\n",
    "    out = llm.invoke(prompt)\n",
    "    expansions = list(set(out.content.strip().split(\"\\n\")))\n",
    "    return [q.strip(\"- \").strip() for q in expansions if q.strip()]\n",
    "\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.3,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "user_question = \"Why did Alice follow the White Rabbit and what happened immediately after she fell down the rabbit hole?\"\n",
    "expansions = expand_queries(llm_gemini, user_question)\n",
    "\n",
    "#printing\n",
    "print(\"### Multi-Query Expansions ###\")\n",
    "print(\"Original:\", user_question)\n",
    "for i, e in enumerate(expansions, 1):\n",
    "    print(f\"Q{i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7c14e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Retrieved Candidates ###\n",
      "Query: Why did Alice follow the White Rabbit and what happened immediately after she fell down the rabbit hole?\n",
      "Score (cosine ANN): 0.2552\n",
      "Meta: source=alice_in_wonderland.md | chapter_number=N/A | chapter_title= | position=-1 | chunk_number=1 | chunk_summary=Alice, feeling sleepy on a hot day, muses whether the pleasure of making a daisy‑chain is worth the trouble of picking daisies when a White Rabbit with pink eyes rushes past her, exclaiming “Oh dear! Oh dear! I shall be late!” and pulling a watch from its waistcoat‑pocket—an odd sight that sparks her curiosity and leads her to chase the rabbit down a large rabbit‑hole. She falls into a deep well, where she notices cupboards and bookshelves lining the walls, and even picks up a jar labeled “ORANGE MARMALADE,” though it is empty, and she carefully places it back to avoid disturbing anyone below.\n",
      "---\n",
      "Query: Why did Alice follow the White Rabbit and what happened immediately after she fell down the rabbit hole?\n",
      "Score (cosine ANN): 0.3198\n",
      "Meta: source=alice_in_wonderland.md | chapter_number=Chapter XII | chapter_title=Alice’s Evidence | position=137732 | chunk_number=86 | chunk_summary=“That _proves_ his guilt,” said the Queen, while Alice retorted, “It proves nothing of the sort!” and the King demanded the White Rabbit read the verses, which began “They told me you had been to her, And mentioned me to him.” The King declared the poem “the most important piece of evidence we’ve heard yet,” and the jury, after hearing Alice’s dismissal, wrote on their slates, “_She_ doesn’t believe there’s an atom of meaning in it.” Thus the scene ends with the court divided over whether the cryptic verses actually prove guilt.\n",
      "---\n",
      "Query: Why did Alice follow the White Rabbit and what happened immediately after she fell down the rabbit hole?\n",
      "Score (cosine ANN): 0.3373\n",
      "Meta: source=alice_in_wonderland.md | chapter_number=N/A | chapter_title= | position=-1 | chunk_number=83 | chunk_summary=Alice watches the White Rabbit fumble over the list, curious about the next witness, and is surprised when the rabbit reads her name aloud, “—for they haven’t got much evidence _yet_,” she muses. In the courtroom, she hurries to the jury‑box, “Here!” cries Alice, tipping it over and scattering the jurymen like a “globe of goldfish”; she apologizes, “Oh, I _beg_ your pardon!” and rushes to restore order, even correcting a lizard that had been turned upside‑down. The King, sternly insisting, declares, “The trial cannot proceed,” and demands all jurymen be back in place, while Alice, bewildered, replies, “Nothing,” to the King’s probing, “Nothing _whatever?_” he persists.\n",
      "---\n",
      "Query: Why did Alice follow the White Rabbit and what happened immediately after she fell down the rabbit hole?\n",
      "Score (cosine ANN): 0.3386\n",
      "Meta: source=alice_in_wonderland.md | chapter_number=Chapter IV | chapter_title=The Rabbit Sends in a Little Bill | position=33555 | chunk_number=20 | chunk_summary=Alice is startled when the Rabbit angrily calls out, “Why, Mary Ann, what _are_ you doing out here? Run home this moment, and fetch me a pair of gloves and a fan! Quick, now!” and she flees, thinking, “He took me for his housemaid,” before sneaking into the Rabbit’s house to find the requested fan and gloves. While she muses, “How queer it seems,” to be delivering messages for a rabbit, she discovers a bottle near a looking‑glass and, despite the absence of a “DRINK ME” label, decides to uncork it, proclaiming, “I know _something_ interesting is sure to happen,” hoping it will make her grow large again. Thus, the passage follows Alice’s frantic quest for the Rabbit’s items and her impulsive experiment with the mysterious bottle.\n",
      "---\n",
      "Query: Why did Alice follow the White Rabbit and what happened immediately after she fell down the rabbit hole?\n",
      "Score (cosine ANN): 0.3401\n",
      "Meta: source=alice_in_wonderland.md | chapter_number=N/A | chapter_title= | position=-1 | chunk_number=8 | chunk_summary=In the passage, Alice laments her own tears, telling herself, “You ought to be ashamed of yourself,” and “a great girl like you,” while a pool of water gathers around her. The White Rabbit then arrives, muttering, “Oh! the Duchess, the Duchess! Oh! won’t she be savage if I’ve kept her waiting!” and, startled, drops his gloves and fan before scurrying away. Alice, fanning herself, muses, “Dear, dear! How queer everything is to-day!” and wonders, “I almost think I can remember feeling a little different,” as she questions whether she has changed overnight.\n",
      "---\n",
      "Query: Could you explain why Alice followed the White Rabbit, and describe her initial experience directly upon falling into the rabbit hole?\n",
      "Score (cosine ANN): 0.2994\n",
      "Meta: source=alice_in_wonderland.md | chapter_number=Chapter I | chapter_title=Down the Rabbit-Hole | position=10581 | chunk_number=6 | chunk_summary=Alice, frustrated after forgetting the golden key and failing to reach it, scolds herself sharply, “Come, there’s no use in crying like that!” and laments that she can’t even pretend to be two people. She discovers a small cake marked “EAT ME” in a glass box, decides to eat it hoping it will make her grow or shrink so she can get the key, but after a bite she finds she remains the same size, remarking that “it seemed quite dull and stupid for life to go on in the common way.” Thus, her attempt to escape the garden ends in disappointment, and she finishes the cake in frustration.\n",
      "---\n",
      "Query: What prompted Alice to chase the White Rabbit, and what were the first things that occurred after she tumbled down the rabbit hole?\n",
      "Score (cosine ANN): 0.3570\n",
      "Meta: source=alice_in_wonderland.md | chapter_number=N/A | chapter_title= | position=-1 | chunk_number=25 | chunk_summary=The scene opens with a chorus of “There goes Bill!” followed by the Rabbit’s voice urging “Catch him, you by the hedge!” and a chaotic exchange of questions about Bill’s condition. Alice hears a feeble voice claiming “That’s Bill,” and the group, spurred by the Rabbit, threatens to burn the house down, prompting Alice to warn, “If you do, I’ll set Dinah at you!” As the Rabbit declares “A barrowful will do, to begin with,” pebbles turn into cakes, and Alice muses, “If I eat one of these cakes, it’s sure to make some change in my size; and as it can’t possibly make me larger, it must make me smaller, I suppose.”\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# STEP 7: Retrieve chunks (cosine similarity) for original + expansions\n",
    "\n",
    "def retrieve_candidates(vectordb, queries, per_query_k=5):\n",
    "    results = []\n",
    "    seen = set()\n",
    "    for q in queries:\n",
    "        hits = vectordb.similarity_search_with_score(q, k=per_query_k)\n",
    "        for doc, score in hits:\n",
    "            key = (doc.metadata[\"start_index\"], doc.metadata[\"chunk_number\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                results.append((doc, score, q))\n",
    "    return results\n",
    "\n",
    "queries = [user_question] + expansions\n",
    "candidates = retrieve_candidates(vectordb, queries, per_query_k=5)\n",
    "\n",
    "# #printing\n",
    "# #printing\n",
    "print(\"### Retrieved Candidates ###\")\n",
    "\n",
    "def _get(m, k, default=\"\"):\n",
    "    return m.get(k, default)\n",
    "\n",
    "for idx, (doc, score, q) in enumerate(candidates[:20], 1):\n",
    "    m = doc.metadata or {}\n",
    "    meta_line = (\n",
    "        f\"source={_get(m,'source')} | \"\n",
    "        f\"chapter_number={_get(m,'chapter_number','N/A')} | \"\n",
    "        f\"chapter_title={_get(m,'chapter_title','')} | \"\n",
    "        f\"position={_get(m,'start_index','-')} | \"\n",
    "        f\"chunk_number={_get(m,'chunk_number','-')} | \"\n",
    "        f\"chunk_summary={_get(m,'chunk_summary','')}\"\n",
    "    )\n",
    "    print(f\"Query: {q}\")\n",
    "    print(f\"Score (cosine ANN): {score:.4f}\")\n",
    "    print(\"Meta:\", meta_line)\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "697a11ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Top Reranked Docs ###\n",
      "RERANK=2.9395 | Unknown Unknown | Alice, feeling sleepy on a hot day, muses whether the pleasure of making a daisy‑chain is worth the ...\n",
      "RERANK=-1.6641 | Chapter IV The Rabbit Sends in a Little Bill | Alice is startled when the Rabbit angrily calls out, “Why, Mary Ann, what _are_ you doing out here? ...\n",
      "RERANK=-1.7793 | Chapter XII Alice’s Evidence | “That _proves_ his guilt,” said the Queen, while Alice retorted, “It proves nothing of the sort!” an...\n",
      "RERANK=-5.1797 | Chapter I Down the Rabbit-Hole | Alice, frustrated after forgetting the golden key and failing to reach it, scolds herself sharply, “...\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: Rerank candidates with cross-encoder (dedupe; take top 5)\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "reranker = FlagReranker(\"BAAI/bge-reranker-base\", use_fp16=True)\n",
    "\n",
    "def rerank_candidates(question, candidates, top_n=5):\n",
    "    pairs = [[question, doc.page_content] for doc, _, _ in candidates]\n",
    "    scores = reranker.compute_score(pairs)\n",
    "    reranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Deduplicate by start_index\n",
    "    seen = set()\n",
    "    top_docs = []\n",
    "    for (doc, score, q), rerank_score in reranked:\n",
    "        if doc.metadata[\"start_index\"] not in seen:\n",
    "            seen.add(doc.metadata[\"start_index\"])\n",
    "            top_docs.append((doc, rerank_score))\n",
    "        if len(top_docs) == top_n:\n",
    "            break\n",
    "    return top_docs\n",
    "\n",
    "top_docs = rerank_candidates(user_question, candidates, top_n=5)\n",
    "\n",
    "# #printing\n",
    "# #printing\n",
    "print(\"### Top Reranked Docs ###\")\n",
    "for d, s in top_docs:\n",
    "    m = d.metadata or {}\n",
    "    chapter_number = m.get('chapter_number', 'Unknown')\n",
    "    chapter_title  = m.get('chapter_title', 'Unknown')\n",
    "    chunk_summary  = (m.get('chunk_summary') or '')[:100]\n",
    "    print(f\"RERANK={s:.4f} | {chapter_number} {chapter_title} | {chunk_summary}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebad5599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### FINAL ANSWER ###\n",
      "Alice followed the White Rabbit because she was overcome with curiosity. She was astonished to see a rabbit take a watch out of its waistcoat-pocket, a sight she had never witnessed before. Immediately after she plunged down the rabbit hole, she found herself falling down a very deep well. During her descent, she had ample time to observe the well's sides, which were lined with cupboards, bookshelves, maps, and pictures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"### Context Headers Preview ###\")\\nfor d, _ in top_docs:\\n    m = d.metadata or {}\\n    print(f\"[Source: {m.get(\\'source\\',\\'unknown\\')} | Chapter {m.get(\\'chapter_number\\',\\'Unknown\\')} {m.get(\\'chapter_title\\',\\'Unknown\\')} | Position={m.get(\\'start_index\\',\\'?\\')} | Chunk {m.get(\\'chunk_number\\',\\'?\\')}]\") '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 9: Build context and answer with Gemini (final result)\n",
    "\n",
    "def build_context(docs):\n",
    "    parts = []\n",
    "    for d, _ in docs:\n",
    "        m = getattr(d, \"metadata\", {}) or {}\n",
    "        source         = m.get(\"source\", \"unknown\")\n",
    "        chapter_number = m.get(\"chapter_number\", \"Unknown\")\n",
    "        chapter_title  = m.get(\"chapter_title\", \"Unknown\")\n",
    "        start_index    = m.get(\"start_index\", \"?\")\n",
    "        chunk_number   = m.get(\"chunk_number\", \"?\")\n",
    "        header = f\"[Source: {source} | Chapter {chapter_number} {chapter_title} | Position={start_index} | Chunk {chunk_number}]\"\n",
    "        parts.append(header + \"\\n\" + d.page_content)\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "context = build_context(top_docs)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. \n",
    "Answer the user question using ONLY the provided context.\n",
    "Read the chunk summary carefully and if it matches with the question then check the chunk content and answer the question.\n",
    "Expand the answer into at least 2–3 sentences and don't use quotes from the content unless the question is asking for the quotes.\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "Context:\n",
    "{context}\n",
    "\"\"\")\n",
    "\n",
    "answer = llm_gemini.invoke(prompt.format(question=user_question, context=context))\n",
    "\n",
    "# #printing\n",
    "print(\"### FINAL ANSWER ###\")\n",
    "print(answer.content)\n",
    "\n",
    "# #printing\n",
    "\"\"\"\n",
    "print(\"### Context Headers Preview ###\")\n",
    "for d, _ in top_docs:\n",
    "    m = d.metadata or {}\n",
    "    print(f\"[Source: {m.get('source','unknown')} | Chapter {m.get('chapter_number','Unknown')} {m.get('chapter_title','Unknown')} | Position={m.get('start_index','?')} | Chunk {m.get('chunk_number','?')}]\") \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
