{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9fef45c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install langchain langchain-google-genai chromadb python-dotenv unstructured sentence-transformers langchain-community --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c154bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to handle tokenizers parallelism\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Explicitly disable parallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ecd407e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and load API key\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and set it in the environment\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY in your .env file\")\n",
    "\n",
    "# Set the API key for Google Generative AI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "50220ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "16fffda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "# Set up paths with absolute references\n",
    "import os\n",
    "\n",
    "# Get absolute paths\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_PATH = os.path.join(current_dir, \"alice_in_wonderland.md\")\n",
    "PERSIST_DIR = os.path.join(current_dir, \"chroma_rag_db\")  # New database directory\n",
    "COLLECTION = \"alice_wonderland\"\n",
    "\n",
    "# Ensure the persist directory exists with proper permissions\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "os.chmod(PERSIST_DIR, 0o755)  # More secure permissions (rwxr-xr-x)\n",
    "\n",
    "# Load and preprocess the document\n",
    "loader = UnstructuredMarkdownLoader(DATA_PATH, show_progress=True)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Configure text splitter for better chunks\n",
    "CHUNK_SIZE = 1000  # Larger chunks to maintain more context\n",
    "CHUNK_OVERLAP = 200  # Larger overlap to prevent losing context at boundaries\n",
    "\n",
    "# Use RecursiveCharacterTextSplitter with better separators\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],  # More granular splitting\n",
    "    keep_separator=True,  # Keep the separators to maintain readability\n",
    "    strip_whitespace=True,  # Clean up whitespace\n",
    "    add_start_index=True,  # Add position info to metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ec91d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 217 chunks, 185 after filtering\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks and filter out boilerplate\n",
    "def is_meaningful_chunk(text: str) -> bool:\n",
    "    # Skip headers, licensing info, and other boilerplate\n",
    "    skip_patterns = [\n",
    "        \"Project Gutenberg\",\n",
    "        \"THE MILLENNIUM FULCRUM EDITION\",\n",
    "        \"Contents\",\n",
    "        \"*      *      *\",\n",
    "        \"trademark\",\n",
    "        \"license\",\n",
    "        \"copyright\"\n",
    "    ]\n",
    "    return not any(pattern.lower() in text.lower() for pattern in skip_patterns)\n",
    "\n",
    "# Split and filter chunks\n",
    "chunks = splitter.split_documents(docs)\n",
    "filtered_chunks = [\n",
    "    chunk for chunk in chunks \n",
    "    if is_meaningful_chunk(chunk.page_content) and len(chunk.page_content.strip()) > 50  # Skip very short chunks\n",
    "]\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks, {len(filtered_chunks)} after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "efaf1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing collection: alice_wonderland\n",
      "Database created and saved to disk with 185 chunks\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings & persist vector DB\n",
    "\n",
    "# Initialize ChromaDB with explicit settings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB client with explicit settings\n",
    "chroma_settings = Settings(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    is_persistent=True,\n",
    "    anonymized_telemetry=False\n",
    ")\n",
    "\n",
    "# Create a new client instance\n",
    "chroma_client = chromadb.Client(chroma_settings)\n",
    "\n",
    "# Get or create collection - this is safer than deleting/recreating\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=COLLECTION)\n",
    "    print(f\"Using existing collection: {COLLECTION}\")\n",
    "except:\n",
    "    collection = chroma_client.create_collection(name=COLLECTION)\n",
    "    print(f\"Created new collection: {COLLECTION}\")\n",
    "\n",
    "# Initialize the sentence-transformers embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Lightweight, fast model\n",
    "model_kwargs = {\n",
    "    'device': 'cpu'  # Use CPU for better compatibility\n",
    "}\n",
    "encode_kwargs = {\n",
    "    'normalize_embeddings': True,  # Normalize for better similarity matching\n",
    "    'batch_size': 32  # Process in smaller batches for memory efficiency\n",
    "}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Create vector DB with filtered chunks\n",
    "# Note: Using normalized embeddings which automatically uses cosine similarity\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=filtered_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    collection_name=COLLECTION,\n",
    "    client=chroma_client  # Use our explicitly configured client\n",
    ")\n",
    "print(f\"Database created and saved to disk with {len(filtered_chunks)} chunks\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f85ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Summarize the entire story?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: Summarize the entire story?\n",
      "\n",
      "Retrieved 2 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "One of the jurors had a pencil that squeaked. This of course, Alice could not stand, and she went round the court and got behind him, and very soon found an opportunity of taking it away. She did it so quickly that the poor little juror (it was Bill, the Lizard) could not make out at all what had become of it; so, after hunting all about for it, he was obliged to write with one finger for the rest of the day; and this was of very little use, as it left no mark on the slate.\n",
      "\n",
      "“Herald, read the accusation!” said the King.\n",
      "\n",
      "On this the White Rabbit blew three blasts on the trumpet, and then unrolled the parchment scroll, and read as follows:—\n",
      "\n",
      "“The Queen of Hearts, she made some tarts, All on a summer day: The Knave of Hearts, he stole those tarts, And took them quite away!”\n",
      "\n",
      "“Consider your verdict,” the King said to the jury.\n",
      "\n",
      "“Not yet, not yet!” the Rabbit hastily interrupted. “There’s a great deal to come before that!”\n",
      "\n",
      "Metadata: {'start_index': 124958, 'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "As soon as the jury had a little recovered from the shock of being upset, and their slates and pencils had been found and handed back to them, they set to work very diligently to write out a history of the accident, all except the Lizard, who seemed too much overcome to do anything but sit with its mouth open, gazing up into the roof of the court.\n",
      "\n",
      "“What do you know about this business?” the King said to Alice.\n",
      "\n",
      "“Nothing,” said Alice.\n",
      "\n",
      "“Nothing whatever?” persisted the King.\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 134057}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL ANSWER ===\n",
      "\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reload persisted DB with consistent settings\n",
    "vectordb = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION,  # Use same collection name\n",
    "    client_settings=chroma_settings  # Use same settings\n",
    ")\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def answer_query(query: str, k: int = 3):\n",
    "    print(\"=== RETRIEVAL STEP ===\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Use MMR for better diversity and relevance\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": k,\n",
    "            \"fetch_k\": k * 3,\n",
    "            \"lambda_mult\": 0.7  # Balance between relevance (1.0) and diversity (0.0)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get initial documents\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Filter for unique content and those with start_index\n",
    "    seen_content = set()\n",
    "    filtered_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Get start_index from metadata if it exists\n",
    "        start_index = doc.metadata.get('start_index', None)\n",
    "        if start_index is None:\n",
    "            continue\n",
    "            \n",
    "        # Normalize content for comparison (remove extra whitespace)\n",
    "        content = ' '.join(doc.page_content.split())\n",
    "        \n",
    "        # Skip if we've seen this content before\n",
    "        if content in seen_content:\n",
    "            continue\n",
    "            \n",
    "        seen_content.add(content)\n",
    "        filtered_docs.append((doc, start_index))\n",
    "    \n",
    "    # Sort by start_index and take top 3\n",
    "    filtered_docs.sort(key=lambda x: x[1])\n",
    "    final_docs = [doc for doc, _ in filtered_docs[:3]]\n",
    "    \n",
    "    # Show retrieved chunks\n",
    "    print(f\"Retrieved {len(final_docs)} unique chunks:\")\n",
    "    for i, doc in enumerate(final_docs):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(\"Content:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"\\nMetadata:\", doc.metadata)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if not final_docs:\n",
    "        print(\"\\nNo relevant chunks found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== GENERATION STEP ===\")\n",
    "    \n",
    "    # Optimized prompt for flash model with narrative context\n",
    "    template = \"\"\"You are helping answer questions about Alice in Wonderland. Use only the provided context to answer. Be detailed in your answer and don't say you can't answer the question if there's retrieved chunks. Summarize the chunks very well.\n",
    "If you can't find the answer in the context, say \"Based on the provided context, I cannot answer this question.\"\n",
    "\n",
    "Context (in story order):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Use only information from the context\n",
    "2. Be specific and quote relevant parts\n",
    "3. Follow the story's sequence when describing events\n",
    "4. If information is incomplete, say so\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Format prompt with better context joining and position info\n",
    "    contexts = []\n",
    "    for doc in final_docs:\n",
    "        # Add position context to help with narrative flow\n",
    "        start_idx = doc.metadata.get('start_index', 0)\n",
    "        context = f\"[Story position {start_idx}]:\\n{doc.page_content}\"\n",
    "        contexts.append(context)\n",
    "    formatted_context = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "    formatted = prompt.format(context=formatted_context, question=query)\n",
    "\n",
    "    # Use Gemini flash model with narrative-optimized settings\n",
    "    chat = ChatGoogleGenerativeAI(\n",
    "        model=\"models/gemini-2.5-pro\",\n",
    "        temperature=0.2,  # Slightly higher for better narrative flow\n",
    "        top_p=0.85,      # More focused token selection\n",
    "        top_k=30,        # More focused selection\n",
    "        max_output_tokens=512  # Limit length for more concise answers\n",
    "    )\n",
    "    response = chat.invoke(formatted)\n",
    "\n",
    "    print(\"\\n=== FINAL ANSWER ===\")\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "# Test queries focusing on specific events/characters\n",
    "queries = [\n",
    "    \"Summarize the entire story?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    answer_query(query)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
