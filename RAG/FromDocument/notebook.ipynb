{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9fef45c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755628425.720722 4712835 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install langchain langchain-google-genai chromadb python-dotenv unstructured --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ecd407e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and load API key\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and set it in the environment\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY in your .env file\")\n",
    "\n",
    "# Set the API key for Google Generative AI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "50220ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "16fffda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"alice_in_wonderland.md\"  # Updated file extension to .md\n",
    "PERSIST_DIR = \"chroma_db\"  # Using a single consistent directory name\n",
    "COLLECTION = \"docs\"\n",
    "\n",
    "# Load and preprocess the document\n",
    "loader = UnstructuredMarkdownLoader(DATA_PATH, show_progress=True)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Configure text splitter for better chunks\n",
    "CHUNK_SIZE = 1000  # Larger chunks to maintain more context\n",
    "CHUNK_OVERLAP = 200  # Larger overlap to prevent losing context at boundaries\n",
    "\n",
    "# Use RecursiveCharacterTextSplitter with better separators\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],  # More granular splitting\n",
    "    keep_separator=True,  # Keep the separators to maintain readability\n",
    "    strip_whitespace=True,  # Clean up whitespace\n",
    "    add_start_index=True,  # Add position info to metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ec91d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 217 chunks, 185 after filtering\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks and filter out boilerplate\n",
    "def is_meaningful_chunk(text: str) -> bool:\n",
    "    # Skip headers, licensing info, and other boilerplate\n",
    "    skip_patterns = [\n",
    "        \"Project Gutenberg\",\n",
    "        \"THE MILLENNIUM FULCRUM EDITION\",\n",
    "        \"Contents\",\n",
    "        \"*      *      *\",\n",
    "        \"trademark\",\n",
    "        \"license\",\n",
    "        \"copyright\"\n",
    "    ]\n",
    "    return not any(pattern.lower() in text.lower() for pattern in skip_patterns)\n",
    "\n",
    "# Split and filter chunks\n",
    "chunks = splitter.split_documents(docs)\n",
    "filtered_chunks = [\n",
    "    chunk for chunk in chunks \n",
    "    if is_meaningful_chunk(chunk.page_content) and len(chunk.page_content.strip()) > 50  # Skip very short chunks\n",
    "]\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks, {len(filtered_chunks)} after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8e27c37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and saved to disk with 185 chunks\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings & persist vector DB\n",
    "\n",
    "# Gemini embeddings with retry on failure\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def get_embeddings():\n",
    "    return GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        task_type=\"retrieval_query\"  # Specify task type for better embeddings\n",
    "    )\n",
    "\n",
    "embeddings = get_embeddings()\n",
    "\n",
    "# Create vector DB with filtered chunks\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=filtered_chunks,  # Use filtered chunks\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    collection_name=COLLECTION\n",
    ")\n",
    "print(f\"Database created and saved to disk with {len(filtered_chunks)} chunks\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2f85ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What happens at the tea party?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: What happens at the tea party?\n",
      "\n",
      "Retrieved 0 unique chunks:\n",
      "\n",
      "No relevant chunks found.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: How does Alice meet the Mad Hatter?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: How does Alice meet the Mad Hatter?\n",
      "\n",
      "Retrieved 3 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "“Oh, you’re sure to do that,” said the Cat, “if you only walk long enough.”\n",
      "\n",
      "Alice felt that this could not be denied, so she tried another question. “What sort of people live about here?”\n",
      "\n",
      "“In that direction,” the Cat said, waving its right paw round, “lives a Hatter: and in that direction,” waving the other paw, “lives a March Hare. Visit either you like: they’re both mad.”\n",
      "\n",
      "“But I don’t want to go among mad people,” Alice remarked.\n",
      "\n",
      "“Oh, you can’t help that,” said the Cat: “we’re all mad here. I’m mad. You’re mad.”\n",
      "\n",
      "“How do you know I’m mad?” said Alice.\n",
      "\n",
      "“You must be,” said the Cat, “or you wouldn’t have come here.”\n",
      "\n",
      "Alice didn’t think that proved it at all; however, she went on “And how do you know that you’re mad?”\n",
      "\n",
      "“To begin with,” said the Cat, “a dog’s not mad. You grant that?”\n",
      "\n",
      "“I suppose so,” said Alice.\n",
      "\n",
      "Metadata: {'start_index': 68988, 'source': 'alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "“Well! I’ve often seen a cat without a grin,” thought Alice; “but a grin without a cat! It’s the most curious thing I ever saw in my life!”\n",
      "\n",
      "She had not gone much farther before she came in sight of the house of the March Hare: she thought it must be the right house, because the chimneys were shaped like ears and the roof was thatched with fur. It was so large a house, that she did not like to go nearer till she had nibbled some more of the lefthand bit of mushroom, and raised herself to about two feet high: even then she walked up towards it rather timidly, saying to herself “Suppose it should be raving mad after all! I almost wish I’d gone to see the Hatter instead!”\n",
      "\n",
      "CHAPTER VII. A Mad Tea-Party\n",
      "\n",
      "Metadata: {'source': 'alice_in_wonderland.md', 'start_index': 71488}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content:\n",
      "“Well, I’d hardly finished the first verse,” said the Hatter, “when the Queen jumped up and bawled out, ‘He’s murdering the time! Off with his head!’”\n",
      "\n",
      "“How dreadfully savage!” exclaimed Alice.\n",
      "\n",
      "“And ever since that,” the Hatter went on in a mournful tone, “he won’t do a thing I ask! It’s always six o’clock now.”\n",
      "\n",
      "A bright idea came into Alice’s head. “Is that the reason so many tea-things are put out here?” she asked.\n",
      "\n",
      "“Yes, that’s it,” said the Hatter with a sigh: “it’s always tea-time, and we’ve no time to wash the things between whiles.”\n",
      "\n",
      "“Then you keep moving round, I suppose?” said Alice.\n",
      "\n",
      "“Exactly so,” said the Hatter: “as the things get used up.”\n",
      "\n",
      "“But what happens when you come to the beginning again?” Alice ventured to ask.\n",
      "\n",
      "“Suppose we change the subject,” the March Hare interrupted, yawning. “I’m getting tired of this. I vote the young lady tells us a story.”\n",
      "\n",
      "“I’m afraid I don’t know one,” said Alice, rather alarmed at the proposal.\n",
      "\n",
      "Metadata: {'source': 'alice_in_wonderland.md', 'start_index': 78522}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "Based on the provided context, Alice learns about the Mad Hatter from the Cheshire Cat, who says,  “In that direction,” the Cat said, waving its right paw round, “lives a Hatter: and in that direction,” waving the other paw, “lives a March Hare. Visit either you like: they’re both mad.”  Later, Alice sees the March Hare's house, which is described as having chimneys shaped like ears and a fur-thatched roof, and considers visiting the Hatter instead.  The provided text then jumps to a Mad Tea-Party with the Hatter and March Hare, but doesn't describe the actual meeting between Alice and the Hatter.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: What does the Queen of Hearts say?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: What does the Queen of Hearts say?\n",
      "\n",
      "Retrieved 1 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "Alice was rather doubtful whether she ought not to lie down on her face like the three gardeners, but she could not remember ever having heard of such a rule at processions; “and besides, what would be the use of a procession,” thought she, “if people had all to lie down upon their faces, so that they couldn’t see it?” So she stood still where she was, and waited.\n",
      "\n",
      "When the procession came opposite to Alice, they all stopped and looked at her, and the Queen said severely “Who is this?” She said it to the Knave of Hearts, who only bowed and smiled in reply.\n",
      "\n",
      "“Idiot!” said the Queen, tossing her head impatiently; and, turning to Alice, she went on, “What’s your name, child?”\n",
      "\n",
      "“My name is Alice, so please your Majesty,” said Alice very politely; but she added, to herself, “Why, they’re only a pack of cards, after all. I needn’t be afraid of them!”\n",
      "\n",
      "Metadata: {'source': 'alice_in_wonderland.md', 'start_index': 87443}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "The Queen of Hearts first says, “Who is this?”  Then, after the Knave of Hearts only bows and smiles, she says, “Idiot!”  Finally, she asks Alice, “What’s your name, child?”\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: Describe the Cheshire Cat's appearance\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: Describe the Cheshire Cat's appearance\n",
      "\n",
      "Retrieved 3 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "Alice was just beginning to think to herself, “Now, what am I to do with this creature when I get it home?” when it grunted again, so violently, that she looked down into its face in some alarm. This time there could be no mistake about it: it was neither more nor less than a pig, and she felt that it would be quite absurd for her to carry it further.\n",
      "\n",
      "So she set the little creature down, and felt quite relieved to see it trot away quietly into the wood. “If it had grown up,” she said to herself, “it would have made a dreadfully ugly child: but it makes rather a handsome pig, I think.” And she began thinking over other children she knew, who might do very well as pigs, and was just saying to herself, “if one only knew the right way to change them—” when she was a little startled by seeing the Cheshire Cat sitting on a bough of a tree a few yards off.\n",
      "\n",
      "Metadata: {'start_index': 67437, 'source': 'alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "“Well! I’ve often seen a cat without a grin,” thought Alice; “but a grin without a cat! It’s the most curious thing I ever saw in my life!”\n",
      "\n",
      "She had not gone much farther before she came in sight of the house of the March Hare: she thought it must be the right house, because the chimneys were shaped like ears and the roof was thatched with fur. It was so large a house, that she did not like to go nearer till she had nibbled some more of the lefthand bit of mushroom, and raised herself to about two feet high: even then she walked up towards it rather timidly, saying to herself “Suppose it should be raving mad after all! I almost wish I’d gone to see the Hatter instead!”\n",
      "\n",
      "CHAPTER VII. A Mad Tea-Party\n",
      "\n",
      "Metadata: {'start_index': 71488, 'source': 'alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content:\n",
      "The players all played at once without waiting for turns, quarrelling all the while, and fighting for the hedgehogs; and in a very short time the Queen was in a furious passion, and went stamping about, and shouting “Off with his head!” or “Off with her head!” about once in a minute.\n",
      "\n",
      "Alice began to feel very uneasy: to be sure, she had not as yet had any dispute with the Queen, but she knew that it might happen any minute, “and then,” thought she, “what would become of me? They’re dreadfully fond of beheading people here; the great wonder is, that there’s any one left alive!”\n",
      "\n",
      "She was looking about for some way of escape, and wondering whether she could get away without being seen, when she noticed a curious appearance in the air: it puzzled her very much at first, but, after watching it a minute or two, she made it out to be a grin, and she said to herself “It’s the Cheshire Cat: now I shall have somebody to talk to.”\n",
      "\n",
      "Metadata: {'start_index': 92640, 'source': 'alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "The Cheshire Cat's first appearance is described as sitting \"on a bough of a tree a few yards off\".  Later, Alice observes \"a curious appearance in the air\" which she identifies as \"a grin\", stating \"It’s the Cheshire Cat\".  Based on the provided context, I cannot fully describe the Cheshire Cat's appearance beyond this grin appearing in the air, separate from its body.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reload persisted DB\n",
    "vectordb = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"docs\"\n",
    ")\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def answer_query(query: str, k: int = 3):\n",
    "    print(\"=== RETRIEVAL STEP ===\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Use MMR retrieval with higher fetch_k for better candidate selection\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": k * 2,  # Fetch more initially for filtering\n",
    "            \"fetch_k\": k * 4,\n",
    "            \"lambda_mult\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get initial documents\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Filter for unique content and those with start_index\n",
    "    seen_content = set()\n",
    "    filtered_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Get start_index from metadata if it exists\n",
    "        start_index = doc.metadata.get('start_index', None)\n",
    "        if start_index is None:\n",
    "            continue\n",
    "            \n",
    "        # Normalize content for comparison (remove extra whitespace)\n",
    "        content = ' '.join(doc.page_content.split())\n",
    "        \n",
    "        # Skip if we've seen this content before\n",
    "        if content in seen_content:\n",
    "            continue\n",
    "            \n",
    "        seen_content.add(content)\n",
    "        filtered_docs.append((doc, start_index))\n",
    "    \n",
    "    # Sort by start_index and take top 3\n",
    "    filtered_docs.sort(key=lambda x: x[1])\n",
    "    final_docs = [doc for doc, _ in filtered_docs[:3]]\n",
    "    \n",
    "    # Show retrieved chunks\n",
    "    print(f\"Retrieved {len(final_docs)} unique chunks:\")\n",
    "    for i, doc in enumerate(final_docs):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(\"Content:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"\\nMetadata:\", doc.metadata)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if not final_docs:\n",
    "        print(\"\\nNo relevant chunks found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== GENERATION STEP ===\")\n",
    "    \n",
    "    # Optimized prompt for flash model with narrative context\n",
    "    template = \"\"\"You are helping answer questions about Alice in Wonderland. Use only the provided context to answer.\n",
    "If you can't find the answer in the context, say \"Based on the provided context, I cannot answer this question.\"\n",
    "\n",
    "Context (in story order):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Use only information from the context\n",
    "2. Be specific and quote relevant parts\n",
    "3. Follow the story's sequence when describing events\n",
    "4. If information is incomplete, say so\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Format prompt with better context joining and position info\n",
    "    contexts = []\n",
    "    for doc in final_docs:\n",
    "        # Add position context to help with narrative flow\n",
    "        start_idx = doc.metadata.get('start_index', 0)\n",
    "        context = f\"[Story position {start_idx}]:\\n{doc.page_content}\"\n",
    "        contexts.append(context)\n",
    "    formatted_context = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "    formatted = prompt.format(context=formatted_context, question=query)\n",
    "\n",
    "    # Use Gemini flash model with narrative-optimized settings\n",
    "    chat = ChatGoogleGenerativeAI(\n",
    "        model=\"models/gemini-1.5-flash\",\n",
    "        temperature=0.2,  # Slightly higher for better narrative flow\n",
    "        top_p=0.85,      # More focused token selection\n",
    "        top_k=30,        # More focused selection\n",
    "        max_output_tokens=512  # Limit length for more concise answers\n",
    "    )\n",
    "    response = chat.invoke(formatted)\n",
    "\n",
    "    print(\"\\n=== FINAL ANSWER ===\")\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "# Test queries focusing on specific events/characters\n",
    "queries = [\n",
    "    \"What happens at the tea party?\",\n",
    "    \"How does Alice meet the Mad Hatter?\",\n",
    "    \"What does the Queen of Hearts say?\",\n",
    "    \"Describe the Cheshire Cat's appearance\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    answer_query(query)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
