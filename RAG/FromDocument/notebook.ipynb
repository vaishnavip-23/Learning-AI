{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "9fef45c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install langchain langchain-google-genai chromadb python-dotenv unstructured sentence-transformers langchain-community --quiet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c154bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to handle tokenizers parallelism\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Explicitly disable parallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ecd407e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and load API key\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key and set it in the environment\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Set GEMINI_API_KEY in your .env file\")\n",
    "\n",
    "# Set the API key for Google Generative AI\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "50220ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "16fffda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "# Set up paths with absolute references\n",
    "import os\n",
    "\n",
    "# Get absolute paths\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "DATA_PATH = os.path.join(current_dir, \"alice_in_wonderland.md\")\n",
    "PERSIST_DIR = os.path.join(current_dir, \"chroma_rag_db\")  # New database directory\n",
    "COLLECTION = \"alice_wonderland\"\n",
    "\n",
    "# Ensure the persist directory exists with proper permissions\n",
    "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "os.chmod(PERSIST_DIR, 0o755)  # More secure permissions (rwxr-xr-x)\n",
    "\n",
    "# Load and preprocess the document\n",
    "loader = UnstructuredMarkdownLoader(DATA_PATH, show_progress=True)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Configure text splitter for better chunks\n",
    "CHUNK_SIZE = 1000  # Larger chunks to maintain more context\n",
    "CHUNK_OVERLAP = 200  # Larger overlap to prevent losing context at boundaries\n",
    "\n",
    "# Use RecursiveCharacterTextSplitter with better separators\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],  # More granular splitting\n",
    "    keep_separator=True,  # Keep the separators to maintain readability\n",
    "    strip_whitespace=True,  # Clean up whitespace\n",
    "    add_start_index=True,  # Add position info to metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "ec91d9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 217 chunks, 185 after filtering\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks and filter out boilerplate\n",
    "def is_meaningful_chunk(text: str) -> bool:\n",
    "    # Skip headers, licensing info, and other boilerplate\n",
    "    skip_patterns = [\n",
    "        \"Project Gutenberg\",\n",
    "        \"THE MILLENNIUM FULCRUM EDITION\",\n",
    "        \"Contents\",\n",
    "        \"*      *      *\",\n",
    "        \"trademark\",\n",
    "        \"license\",\n",
    "        \"copyright\"\n",
    "    ]\n",
    "    return not any(pattern.lower() in text.lower() for pattern in skip_patterns)\n",
    "\n",
    "# Split and filter chunks\n",
    "chunks = splitter.split_documents(docs)\n",
    "filtered_chunks = [\n",
    "    chunk for chunk in chunks \n",
    "    if is_meaningful_chunk(chunk.page_content) and len(chunk.page_content.strip()) > 50  # Skip very short chunks\n",
    "]\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks, {len(filtered_chunks)} after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "efaf1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing collection: alice_wonderland\n",
      "Database created and saved to disk with 185 chunks\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings & persist vector DB\n",
    "\n",
    "# Initialize ChromaDB with explicit settings\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB client with explicit settings\n",
    "chroma_settings = Settings(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    is_persistent=True,\n",
    "    anonymized_telemetry=False\n",
    ")\n",
    "\n",
    "# Create a new client instance\n",
    "chroma_client = chromadb.Client(chroma_settings)\n",
    "\n",
    "# Get or create collection - this is safer than deleting/recreating\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=COLLECTION)\n",
    "    print(f\"Using existing collection: {COLLECTION}\")\n",
    "except:\n",
    "    collection = chroma_client.create_collection(name=COLLECTION)\n",
    "    print(f\"Created new collection: {COLLECTION}\")\n",
    "\n",
    "# Initialize the sentence-transformers embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"  # Lightweight, fast model\n",
    "model_kwargs = {\n",
    "    'device': 'cpu'  # Use CPU for better compatibility\n",
    "}\n",
    "encode_kwargs = {\n",
    "    'normalize_embeddings': True,  # Normalize for better similarity matching\n",
    "    'batch_size': 32  # Process in smaller batches for memory efficiency\n",
    "}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Create vector DB with filtered chunks\n",
    "# Note: Using normalized embeddings which automatically uses cosine similarity\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=filtered_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    collection_name=COLLECTION,\n",
    "    client=chroma_client  # Use our explicitly configured client\n",
    ")\n",
    "print(f\"Database created and saved to disk with {len(filtered_chunks)} chunks\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "2f85ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What happens at the tea party?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: What happens at the tea party?\n",
      "\n",
      "Retrieved 3 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "CHAPTER VII. A Mad Tea-Party\n",
      "\n",
      "There was a table set out under a tree in front of the house, and the March Hare and the Hatter were having tea at it: a Dormouse was sitting between them, fast asleep, and the other two were using it as a cushion, resting their elbows on it, and talking over its head. “Very uncomfortable for the Dormouse,” thought Alice; “only, as it’s asleep, I suppose it doesn’t mind.”\n",
      "\n",
      "Metadata: {'start_index': 72167, 'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "“At any rate I’ll never go there again!” said Alice as she picked her way through the wood. “It’s the stupidest tea-party I ever was at in all my life!”\n",
      "\n",
      "Just as she said this, she noticed that one of the trees had a door leading right into it. “That’s very curious!” she thought. “But everything’s curious today. I think I may as well go in at once.” And in she went.\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 83928}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content:\n",
      "“Call the first witness,” said the King; and the White Rabbit blew three blasts on the trumpet, and called out, “First witness!”\n",
      "\n",
      "The first witness was the Hatter. He came in with a teacup in one hand and a piece of bread-and-butter in the other. “I beg pardon, your Majesty,” he began, “for bringing these in: but I hadn’t quite finished my tea when I was sent for.”\n",
      "\n",
      "“You ought to have finished,” said the King. “When did you begin?”\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 125892}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "At the tea party, the March Hare and the Hatter were having tea under a tree. A Dormouse was sitting between them, asleep, and they were using the table as a cushion. Alice thought it was uncomfortable for the Dormouse. Alice later said it was the \"stupidest tea-party I ever was at in all my life!\" and then went into a tree that had a door leading into it. The Hatter was called as the first witness and brought in a teacup and a piece of bread-and-butter, explaining he hadn't finished his tea.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: How does Alice meet the Mad Hatter?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: How does Alice meet the Mad Hatter?\n",
      "\n",
      "Retrieved 3 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "Alice waited a little, half expecting to see it again, but it did not appear, and after a minute or two she walked on in the direction in which the March Hare was said to live. “I’ve seen hatters before,” she said to herself; “the March Hare will be much the most interesting, and perhaps as this is May it won’t be raving mad—at least not so mad as it was in March.” As she said this, she looked up, and there was the Cat again, sitting on a branch of a tree.\n",
      "\n",
      "Metadata: {'start_index': 70674, 'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "“Is that the way you manage?” Alice asked.\n",
      "\n",
      "The Hatter shook his head mournfully. “Not I!” he replied. “We quarrelled last March—just before he went mad, you know—” (pointing with his tea spoon at the March Hare,) “—it was at the great concert given by the Queen of Hearts, and I had to sing\n",
      "\n",
      "‘Twinkle, twinkle, little bat! How I wonder what you’re at!’\n",
      "\n",
      "You know the song, perhaps?”\n",
      "\n",
      "“I’ve heard something like it,” said Alice.\n",
      "\n",
      "“It goes on, you know,” the Hatter continued, “in this way:—\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 77787}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content:\n",
      "“How dreadfully savage!” exclaimed Alice.\n",
      "\n",
      "“And ever since that,” the Hatter went on in a mournful tone, “he won’t do a thing I ask! It’s always six o’clock now.”\n",
      "\n",
      "A bright idea came into Alice’s head. “Is that the reason so many tea-things are put out here?” she asked.\n",
      "\n",
      "“Yes, that’s it,” said the Hatter with a sigh: “it’s always tea-time, and we’ve no time to wash the things between whiles.”\n",
      "\n",
      "“Then you keep moving round, I suppose?” said Alice.\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 78674}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "Alice meets the Mad Hatter after she has walked in the direction in which the March Hare was said to live. Alice says, \"I’ve seen hatters before,\" and thinks the March Hare will be \"much the most interesting.\" The Hatter then mentions that he quarrelled with the March Hare last March at a concert given by the Queen of Hearts. Alice asks the Hatter \"How dreadfully savage!\" and the Hatter explains that \"And ever since that, he won’t do a thing I ask! It’s always six o’clock now.\" Alice then asks if that's the reason so many tea-things are put out.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: What does the Queen of Hearts say?\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: What does the Queen of Hearts say?\n",
      "\n",
      "Retrieved 3 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      "When the procession came opposite to Alice, they all stopped and looked at her, and the Queen said severely “Who is this?” She said it to the Knave of Hearts, who only bowed and smiled in reply.\n",
      "\n",
      "“Idiot!” said the Queen, tossing her head impatiently; and, turning to Alice, she went on, “What’s your name, child?”\n",
      "\n",
      "“My name is Alice, so please your Majesty,” said Alice very politely; but she added, to herself, “Why, they’re only a pack of cards, after all. I needn’t be afraid of them!”\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 87811}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "“How do you like the Queen?” said the Cat in a low voice.\n",
      "\n",
      "“Not at all,” said Alice: “she’s so extremely—” Just then she noticed that the Queen was close behind her, listening: so she went on, “—likely to win, that it’s hardly worth while finishing the game.”\n",
      "\n",
      "The Queen smiled and passed on.\n",
      "\n",
      "“Who are you talking to?” said the King, going up to Alice, and looking at the Cat’s head with great curiosity.\n",
      "\n",
      "“It’s a friend of mine—a Cheshire Cat,” said Alice: “allow me to introduce it.”\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 94606}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content:\n",
      "“Herald, read the accusation!” said the King.\n",
      "\n",
      "On this the White Rabbit blew three blasts on the trumpet, and then unrolled the parchment scroll, and read as follows:—\n",
      "\n",
      "“The Queen of Hearts, she made some tarts, All on a summer day: The Knave of Hearts, he stole those tarts, And took them quite away!”\n",
      "\n",
      "“Consider your verdict,” the King said to the jury.\n",
      "\n",
      "“Not yet, not yet!” the Rabbit hastily interrupted. “There’s a great deal to come before that!”\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 125438}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "Based on the provided context, I cannot answer this question. The context describes the Queen's behavior and the accusations made against her, but it does not contain any direct quotes of what the Queen of Hearts says.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Query: Describe the Cheshire Cat's appearance\n",
      "================================================================================\n",
      "=== RETRIEVAL STEP ===\n",
      "Query: Describe the Cheshire Cat's appearance\n",
      "\n",
      "Retrieved 3 unique chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Content:\n",
      ".” And she began thinking over other children she knew, who might do very well as pigs, and was just saying to herself, “if one only knew the right way to change them—” when she was a little startled by seeing the Cheshire Cat sitting on a bough of a tree a few yards off.\n",
      "\n",
      "Metadata: {'start_index': 68027, 'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Content:\n",
      "The Cat only grinned when it saw Alice. It looked good-natured, she thought: still it had very long claws and a great many teeth, so she felt that it ought to be treated with respect.\n",
      "\n",
      "“Cheshire Puss,” she began, rather timidly, as she did not at all know whether it would like the name: however, it only grinned a little wider. “Come, it’s pleased so far,” thought Alice, and she went on. “Would you tell me, please, which way I ought to go from here?”\n",
      "\n",
      "Metadata: {'start_index': 68301, 'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Content:\n",
      "She was looking about for some way of escape, and wondering whether she could get away without being seen, when she noticed a curious appearance in the air: it puzzled her very much at first, but, after watching it a minute or two, she made it out to be a grin, and she said to herself “It’s the Cheshire Cat: now I shall have somebody to talk to.”\n",
      "\n",
      "“How are you getting on?” said the Cat, as soon as there was mouth enough for it to speak with.\n",
      "\n",
      "Metadata: {'source': '/Users/vaishnavipullakhandam/Desktop/github/Staying Relevant/RAG/FromDocument/alice_in_wonderland.md', 'start_index': 93225}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== GENERATION STEP ===\n",
      "\n",
      "=== FINAL ANSWER ===\n",
      "The Cheshire Cat is sitting on a bough of a tree. It has very long claws and a great many teeth. It grinned when Alice saw it, and it grinned a little wider when Alice addressed it. Alice noticed a curious appearance in the air that she made out to be a grin. \n",
      "\n",
      "Based on the provided context, I cannot answer this question.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reload persisted DB with consistent settings\n",
    "vectordb = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION,  # Use same collection name\n",
    "    client_settings=chroma_settings  # Use same settings\n",
    ")\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def answer_query(query: str, k: int = 3):\n",
    "    print(\"=== RETRIEVAL STEP ===\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Use MMR for better diversity and relevance\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": k,\n",
    "            \"fetch_k\": k * 3,\n",
    "            \"lambda_mult\": 0.7  # Balance between relevance (1.0) and diversity (0.0)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get initial documents\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Filter for unique content and those with start_index\n",
    "    seen_content = set()\n",
    "    filtered_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Get start_index from metadata if it exists\n",
    "        start_index = doc.metadata.get('start_index', None)\n",
    "        if start_index is None:\n",
    "            continue\n",
    "            \n",
    "        # Normalize content for comparison (remove extra whitespace)\n",
    "        content = ' '.join(doc.page_content.split())\n",
    "        \n",
    "        # Skip if we've seen this content before\n",
    "        if content in seen_content:\n",
    "            continue\n",
    "            \n",
    "        seen_content.add(content)\n",
    "        filtered_docs.append((doc, start_index))\n",
    "    \n",
    "    # Sort by start_index and take top 3\n",
    "    filtered_docs.sort(key=lambda x: x[1])\n",
    "    final_docs = [doc for doc, _ in filtered_docs[:3]]\n",
    "    \n",
    "    # Show retrieved chunks\n",
    "    print(f\"Retrieved {len(final_docs)} unique chunks:\")\n",
    "    for i, doc in enumerate(final_docs):\n",
    "        print(f\"\\n--- Chunk {i+1} ---\")\n",
    "        print(\"Content:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"\\nMetadata:\", doc.metadata)\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if not final_docs:\n",
    "        print(\"\\nNo relevant chunks found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== GENERATION STEP ===\")\n",
    "    \n",
    "    # Optimized prompt for flash model with narrative context\n",
    "    template = \"\"\"You are helping answer questions about Alice in Wonderland. Use only the provided context to answer.\n",
    "If you can't find the answer in the context, say \"Based on the provided context, I cannot answer this question.\"\n",
    "\n",
    "Context (in story order):\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Use only information from the context\n",
    "2. Be specific and quote relevant parts\n",
    "3. Follow the story's sequence when describing events\n",
    "4. If information is incomplete, say so\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Format prompt with better context joining and position info\n",
    "    contexts = []\n",
    "    for doc in final_docs:\n",
    "        # Add position context to help with narrative flow\n",
    "        start_idx = doc.metadata.get('start_index', 0)\n",
    "        context = f\"[Story position {start_idx}]:\\n{doc.page_content}\"\n",
    "        contexts.append(context)\n",
    "    formatted_context = \"\\n\\n---\\n\\n\".join(contexts)\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
    "    formatted = prompt.format(context=formatted_context, question=query)\n",
    "\n",
    "    # Use Gemini flash model with narrative-optimized settings\n",
    "    chat = ChatGoogleGenerativeAI(\n",
    "        model=\"models/gemma-3n-e2b-it\",\n",
    "        temperature=0.2,  # Slightly higher for better narrative flow\n",
    "        top_p=0.85,      # More focused token selection\n",
    "        top_k=30,        # More focused selection\n",
    "        max_output_tokens=512  # Limit length for more concise answers\n",
    "    )\n",
    "    response = chat.invoke(formatted)\n",
    "\n",
    "    print(\"\\n=== FINAL ANSWER ===\")\n",
    "    print(response.content)\n",
    "\n",
    "\n",
    "# Test queries focusing on specific events/characters\n",
    "queries = [\n",
    "    \"What happens at the tea party?\",\n",
    "    \"How does Alice meet the Mad Hatter?\",\n",
    "    \"What does the Queen of Hearts say?\",\n",
    "    \"Describe the Cheshire Cat's appearance\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    answer_query(query)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
