{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d15a9d9",
   "metadata": {},
   "source": [
    "Understand Query Translation through code\n",
    "\n",
    "![Query Translation Pipeline](query_translation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b48fc",
   "metadata": {},
   "source": [
    "We don’t know beforehand if the generated questions will exactly match the document contents.\n",
    "Your original query might be too narrow.\n",
    "By generating multiple versions, you’re casting a wider net.\n",
    "Even if 2–3 versions don’t match the document well, at least 1–2 of them are more likely to hit the right context in the vector store.\n",
    "This is like searching in Google:\n",
    "If you type one phrasing, maybe you miss the right page.\n",
    "If you try synonyms or related phrasing, you have a better chance of finding the right result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e8bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Multi Query \n",
    "\"\"\"\n",
    "Instead of asking one question, we generate many variations of the same question. Unique retrieved documents are combined to form the final answer.\n",
    "\"\"\"\n",
    "\n",
    "#2 RAG Fusion\n",
    "\"\"\"Here we merge results from different query variations (like in multi-query). remove duplicates and combine the results.\"\"\"\n",
    "\n",
    "#3. Decomposition \n",
    "\"\"\"\n",
    "We break down complex questions into simpler sub-questions.\n",
    "1. Either give the answer for one sub question recursively to the next sub question as context. \n",
    "2. generate the answer for all sub questions and then combine the results to LLM.\n",
    "\"\"\"\n",
    "\n",
    "#4. Step Back Prompting\n",
    "\"\"\"\n",
    "Instead of directly answering, we first ask a more general version of the question.\n",
    "We can use few shot prompting for this, to know the history of the question or a more general version of the question.\n",
    "Example:\n",
    "\n",
    "Original: “What are the side effects of Paracetamol in children under 5?”\n",
    "\n",
    "Step-back: “What are the general side effects of Paracetamol?”\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#5. HyDE (Hypothetical Document Embedding)\n",
    "\"\"\"\n",
    "We ask the LLM (Gemini in your case) to write a fake answer to the query.\n",
    "\n",
    "Then we embed this fake answer and use it to search in the database.\n",
    "\n",
    "Example:\n",
    "\n",
    "Query: “Explain photosynthesis.”\n",
    "\n",
    "LLM writes a short fake doc: “Photosynthesis is how plants use sunlight…”\n",
    "\n",
    "Embed that doc → search DB → find real docs.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b704eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
