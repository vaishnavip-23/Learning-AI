{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9041297",
   "metadata": {},
   "source": [
    "RAG using LangChain\n",
    "\n",
    "![RAG Pipeline](download.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f311c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755535635.748401 3900356 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# packages to install\n",
    "\n",
    "%pip install langchain langchain-core langchain-community langchainhub \\\n",
    "            langchain-google-genai chromadb tiktoken python-dotenv bs4 --quiet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e229933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify Gemini API key is set\n",
    "google_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"Please set GEMINI_API_KEY environment variable in .env file\")\n",
    "\n",
    "# Set the API key for Google's services\n",
    "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88184d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Load and clean document\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mload_webpage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;241m=\u001b[39m clean_text(docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Print first document to verify content\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 38\u001b[0m, in \u001b[0;36mload_webpage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m article\u001b[38;5;241m.\u001b[39mfind_all([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     36\u001b[0m         content\u001b[38;5;241m.\u001b[39mappend(elem\u001b[38;5;241m.\u001b[39mget_text())\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mDocument\u001b[49m(page_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(content))]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Document' is not defined"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Gemini integrations\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Custom loader for better content extraction\n",
    "def load_webpage():\n",
    "    url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the main content\n",
    "    content = []\n",
    "    \n",
    "    # Get title\n",
    "    if soup.find('h1'):\n",
    "        content.append(soup.find('h1').get_text())\n",
    "    \n",
    "    # Get main article content\n",
    "    article = soup.find('article') or soup.find('div', class_='post-content')\n",
    "    if article:\n",
    "        # Get all text content\n",
    "        for elem in article.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'li']):\n",
    "            content.append(elem.get_text())\n",
    "    \n",
    "    return [Document(page_content='\\n\\n'.join(content))]\n",
    "\n",
    "# Add custom preprocessing to clean the text\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    # Remove multiple newlines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s\\-.,?!()\\[\\]{}\"\\']+', ' ', text)\n",
    "    return text.strip()\n",
    "# Load and clean document\n",
    "docs = load_webpage()\n",
    "docs[0].page_content = clean_text(docs[0].page_content)\n",
    "\n",
    "# Print first document to verify content\n",
    "print(\"Document content preview:\")\n",
    "print(docs[0].page_content[:500])\n",
    "print(\"\\nTotal documents loaded:\", len(docs))\n",
    "\n",
    "# Split with larger chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(\"\\nTotal splits:\", len(splits))\n",
    "\n",
    "# Embed with Gemini\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
    ")\n",
    "\n",
    "# Configure retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 6}  # Number of documents to return\n",
    ")\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt (still use the same hub prompt)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM â†’ Gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Print a few example chunks to verify content\n",
    "print(\"\\nExample chunks:\")\n",
    "for i, split in enumerate(splits[:2]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(split.page_content[:200])\n",
    "\n",
    "# Question about Task Decomposition\n",
    "query = \"Based on the article about LLM Powered Autonomous Agents, explain how agents handle task decomposition, planning, and breaking down complex tasks into subtasks. Include specific examples if mentioned in the text.\"\n",
    "\n",
    "# Get the response\n",
    "print(\"\\nQuerying about Task Decomposition:\")\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(\"\\nResponse:\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
