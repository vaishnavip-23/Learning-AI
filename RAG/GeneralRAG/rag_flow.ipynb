{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1be4fa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "%pip install --upgrade google-genai sentence-transformers faiss-cpu langchain-text-splitters python-dotenv --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e25f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import os,math, uuid\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer # for local embedding\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter #simple chunker \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY is not set\")\n",
    "\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "GEMINI_GEN_MODEL=\"gemini-2.5-flash\" #fast, good for RAG\n",
    "\n",
    "LOCAL_EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # small, fast, good quality\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44729c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Example on which we'll perform retrieveal \n",
    "# Each doc has: id, title, text, and optional metadata.\n",
    "DOCUMENTS = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Baking Basics\",\n",
    "        \"text\": \"Cakes are baked at moderate temperatures. Common ingredients are flour, sugar, eggs, and butter. Icing is added after the cake cools.\",\n",
    "        \"meta\": {\"source\": \"kitchen-notes\", \"lang\": \"en\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Healthy Desserts\",\n",
    "        \"text\": \"For a lighter dessert, substitute part of the sugar with fruit purees. Consider whole-grain flour. Yogurt frostings can reduce fat.\",\n",
    "        \"meta\": {\"source\": \"health-blog\", \"lang\": \"en\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Birthday Traditions\",\n",
    "        \"text\": \"Many cultures celebrate birthdays with a sweet cake, candles, and a wish. Popular flavors include chocolate and vanilla.\",\n",
    "        \"meta\": {\"source\": \"culture-wiki\", \"lang\": \"en\"}\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aba68364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Chunking \n",
    "# Goal: split long docs into chunks so retrieval can target the right part.\n",
    "# We’ll use a character-based splitter with a small overlap.\n",
    "\n",
    "def chunk_documents(docs:List[Dict],chunk_size,chunk_overlap):\n",
    "    # Returns: list of chunk dicts with fields: chunk_id, doc_id, text, meta\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    chunks=[]\n",
    "    for doc in docs:\n",
    "        parts = splitter.split_text(doc[\"text\"])\n",
    "        for i,part in enumerate(parts):\n",
    "            chunks.append({\n",
    "                \"chunk_id\":f\"{doc['id']}::chunk{i}\",\n",
    "                \"doc_id\":doc[\"id\"],\n",
    "                \"text\":part,\n",
    "                \"meta\":doc.get(\"meta\",{}),\n",
    "                \"title\":doc.get(\"title\",\"\")\n",
    "            })\n",
    "            \n",
    "    return chunks\n",
    "CHUNKS = chunk_documents(DOCUMENTS,chunk_size=100,chunk_overlap=40)\n",
    "print(len(CHUNKS))\n",
    "# print(CHUNKS[0],end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38569da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings and storing it in FAISS (vector store)\n",
    "\n",
    "local_embedder = SentenceTransformer(LOCAL_EMBED_MODEL)\n",
    "\n",
    "def embed_texts(texts:List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Turn a list of texts into dense vectors.\n",
    "    Returns a 2D numpy array: shape (N, D)\n",
    "    \"\"\"\n",
    "    vecs = local_embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return vecs.astype(\"float32\")\n",
    "\n",
    "# Build a FAISS Index\n",
    "@dataclass\n",
    "class VectorIndex:\n",
    "    \"\"\"\n",
    "    Holds:\n",
    "      - faiss index (for fast vector similarity)\n",
    "      - id_to_chunk (mapping from FAISS row -> our chunk dict)\n",
    "      - dim (vector dimension)\n",
    "    \"\"\"\n",
    "    index: faiss.IndexFlatIP\n",
    "    id_to_chunk: Dict[int, Dict]\n",
    "    dim: int\n",
    "\n",
    "def build_faiss_index(chunks: List[Dict]) -> VectorIndex:\n",
    "    \"\"\"\n",
    "    1) Embed each chunk\n",
    "    2) Build a FAISS IndexFlatIP (cosine similarity if vectors are normalized)\n",
    "    3) Add vectors to the index in the same order as id_to_chunk keys\n",
    "    \"\"\"\n",
    "    texts = [c[\"text\"] for c in chunks]\n",
    "    vecs = embed_texts(texts)       # Step 1: embed\n",
    "    dim = vecs.shape[1]             # Step 2: get vector dimension\n",
    "\n",
    "    index = faiss.IndexFlatIP(dim)  # Step 3: build index (Inner Product == cosine if normalized)\n",
    "    index.add(vecs)                 # Step 4: add vectors\n",
    "\n",
    "    id_to_chunk = {i: chunks[i] for i in range(len(chunks))}  # mapping row → chunk\n",
    "    return VectorIndex(index=index, id_to_chunk=id_to_chunk, dim=dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6fe93a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built index with 6 chunks\n"
     ]
    }
   ],
   "source": [
    "# Build the vector index\n",
    "vindex = build_faiss_index(CHUNKS)\n",
    "print(f\"Built index with {len(CHUNKS)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "648c5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval (top-k search) + simple formatting for the LLM\n",
    "def search(vindex: VectorIndex, query: str, k: int = 4) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    1) Embed the query the same way as documents\n",
    "    2) Search top-k vectors in FAISS\n",
    "    3) Return the matching chunk dicts with scores\n",
    "    \"\"\"\n",
    "    qvec = embed_texts([query])\n",
    "\n",
    "    scores, ids = vindex.index.search(qvec, k)\n",
    "    results = []\n",
    "    for rank, (sid, score) in enumerate(zip(ids[0], scores[0])):\n",
    "        if sid == -1:\n",
    "            continue\n",
    "        ch = vindex.id_to_chunk[sid].copy()\n",
    "        ch[\"score\"] = float(score)\n",
    "        ch[\"rank\"] = rank\n",
    "        results.append(ch)\n",
    "    return results\n",
    "\n",
    "def make_rag_prompt(question: str, contexts: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Build a plain prompt for Gemini:\n",
    "    - Include instructions\n",
    "    - Include citations with titles or doc_ids\n",
    "    - Keep it short and clear\n",
    "    \"\"\"\n",
    "    header = (\n",
    "        \"You are a helpful assistant. Answer the question using ONLY the context.\\n\"\n",
    "        \"If the answer is not in the context, say you don't know.\\n\"\n",
    "        \"Cite sources as [title or doc_id].\\n\\n\"\n",
    "    )\n",
    "    ctx_lines = []\n",
    "    for i, c in enumerate(contexts):\n",
    "        tag = c[\"title\"] or c[\"doc_id\"]\n",
    "        ctx_lines.append(f\"[{i+1}:{tag}] {c['text']}\")\n",
    "    ctx_block = \"\\n\".join(ctx_lines)\n",
    "\n",
    "    q_block = f\"\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    return header + ctx_block + q_block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c071b81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " Many cultures celebrate birthdays with a sweet cake, candles, and a wish [1:Birthday Traditions]. \n",
      "\n",
      "CONTEXT USED:\n",
      "0 0.715 Birthday Traditions\n",
      "1 0.444 Baking Basics\n",
      "2 0.399 Baking Basics\n"
     ]
    }
   ],
   "source": [
    "# Generate the answer with Gemini\n",
    "\n",
    "def ask_gemini(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Send a single prompt to Gemini and return the text output.\n",
    "    We add light safety settings to be safe in public apps.\n",
    "    \"\"\"\n",
    "    safety = [\n",
    "        types.SafetySetting(\n",
    "            category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "        ),\n",
    "        types.SafetySetting(\n",
    "            category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "            threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "        ),\n",
    "    ]\n",
    "    config = types.GenerateContentConfig(\n",
    "        safety_settings=safety,\n",
    "        temperature=0.3,   # lower temp for grounded answers\n",
    "    )\n",
    "    resp = client.models.generate_content(\n",
    "        model=GEMINI_GEN_MODEL,\n",
    "        contents=[types.Content(parts=[types.Part(text=prompt)])],\n",
    "        config=config\n",
    "    )\n",
    "    # different SDK versions expose either .text or candidates\n",
    "    if hasattr(resp, \"text\") and resp.text:\n",
    "        return resp.text\n",
    "    if resp.candidates and resp.candidates[0].content.parts:\n",
    "        return resp.candidates[0].content.parts[0].text\n",
    "    return \"(no text returned)\"\n",
    "\n",
    "def basic_rag_answer(question: str, k: int = 4) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    1) Retrieve top-k chunks\n",
    "    2) Make a prompt with those chunks\n",
    "    3) Ask Gemini\n",
    "    Returns (answer_text, contexts_used)\n",
    "    \"\"\"\n",
    "    hits = search(vindex, question, k=k)\n",
    "    prompt = make_rag_prompt(question, hits)\n",
    "    answer = ask_gemini(prompt)\n",
    "    return answer, hits\n",
    "\n",
    "# Try it\n",
    "q = \"How do people usually celebrate birthdays with cake?\"\n",
    "ans, used = basic_rag_answer(q, k=3)\n",
    "print(\"ANSWER:\\n\", ans, \"\\n\")\n",
    "print(\"CONTEXT USED:\")\n",
    "for c in used:\n",
    "    print(c[\"rank\"], round(c[\"score\"], 3), c[\"title\"] or c[\"doc_id\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
