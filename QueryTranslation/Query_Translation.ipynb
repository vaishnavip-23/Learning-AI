{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d15a9d9",
   "metadata": {},
   "source": [
    "Understand Query Translation through code\n",
    "\n",
    "![Query Translation Pipeline](query_translation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b48fc",
   "metadata": {},
   "source": [
    "We don’t know beforehand if the generated questions will exactly match the document contents.\n",
    "Your original query might be too narrow.\n",
    "By generating multiple versions, you’re casting a wider net.\n",
    "Even if 2–3 versions don’t match the document well, at least 1–2 of them are more likely to hit the right context in the vector store.\n",
    "This is like searching in Google:\n",
    "If you type one phrasing, maybe you miss the right page.\n",
    "If you try synonyms or related phrasing, you have a better chance of finding the right result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e1e8bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe ask the LLM (Gemini in your case) to write a fake answer to the query.\\n\\nThen we embed this fake answer and use it to search in the database.\\n\\nExample:\\n\\nQuery: “Explain photosynthesis.”\\n\\nLLM writes a short fake doc: “Photosynthesis is how plants use sunlight…”\\n\\nEmbed that doc → search DB → find real docs.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Multi Query \n",
    "\"\"\"\n",
    "Instead of asking one question, we generate many variations of the same question. Unique retrieved documents are combined to form the final answer.\n",
    "\"\"\"\n",
    "\n",
    "#2 RAG Fusion\n",
    "\"\"\"Here we merge results from different query variations (like in multi-query). remove duplicates and combine the results.\"\"\"\n",
    "\n",
    "#3. Decomposition \n",
    "\"\"\"\n",
    "We break down complex questions into simpler sub-questions.\n",
    "1. Either give the answer for one sub question recursively to the next sub question as context. \n",
    "2. generate the answer for all sub questions and then combine the results to LLM.\n",
    "\"\"\"\n",
    "\n",
    "#4. Step Back Prompting\n",
    "\"\"\"\n",
    "Instead of directly answering, we first ask a more general version of the question.\n",
    "We can use few shot prompting for this, to know the history of the question or a more general version of the question.\n",
    "Example:\n",
    "\n",
    "Original: “What are the side effects of Paracetamol in children under 5?”\n",
    "\n",
    "Step-back: “What are the general side effects of Paracetamol?”\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#5. HyDE (Hypothetical Document Embedding)\n",
    "\"\"\"\n",
    "We ask the LLM (Gemini in your case) to write a fake answer to the query.\n",
    "\n",
    "Then we embed this fake answer and use it to search in the database.\n",
    "\n",
    "Example:\n",
    "\n",
    "Query: “Explain photosynthesis.”\n",
    "\n",
    "LLM writes a short fake doc: “Photosynthesis is how plants use sunlight…”\n",
    "\n",
    "Embed that doc → search DB → find real docs.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de2ab59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/Users/vaishnavipullakhandam/anaconda3/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain langchain-google-genai sentence-transformers chromadb python-dotenv --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b05d6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20e1f159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup Gemini LLM (using .env key)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Local embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36dc812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Query Example:\n",
      "['1. Which city serves as the capital of the United States?', '2. Where is the capital of America located?', \"3. What's the name of the city that's the capital of the United States of America?\"]\n"
     ]
    }
   ],
   "source": [
    "# 1. MULTI QUERY\n",
    "def generate_multi_queries(question):\n",
    "    \"\"\"Generate multiple variations of a question using Gemini.\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Generate 3 different rephrasings of the question:\\n{question}\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run({\"question\": question})\n",
    "    queries = result.split(\"\\n\")\n",
    "    return [q.strip(\"- \").strip() for q in queries if q.strip()]\n",
    "\n",
    "print(\"Multi-Query Example:\")\n",
    "print(generate_multi_queries(\"What is the capital of America?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90a33489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RAG-FUSION\n",
    "\n",
    "def rag_fusion(question, retriever):\n",
    "    \"\"\"Ask multiple queries, combine all retrieved docs.\"\"\"\n",
    "    queries = generate_multi_queries(question)\n",
    "    all_docs = []\n",
    "    for q in queries:\n",
    "        docs = retriever(q)  # Assume retriever is a function returning docs\n",
    "        all_docs.extend(docs)\n",
    "    # Deduplicate by page content\n",
    "    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "    return unique_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e642a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decomposition Example:\n",
      "['1. How did World War II impact US industrial production and employment?', '2. What were the long-term effects of WWII on US government spending and debt?', \"3. How did World War II reshape the US's role in the global economy?\"]\n"
     ]
    }
   ],
   "source": [
    "# 3. DECOMPOSITION\n",
    "def decompose_question(question):\n",
    "    \"\"\"Break a complex question into smaller sub-questions.\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Break this question into 3 smaller questions:\\n{question}\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run({\"question\": question})\n",
    "    return [q.strip(\"- \").strip() for q in result.split(\"\\n\") if q.strip()]\n",
    "\n",
    "print(\"\\nDecomposition Example:\")\n",
    "print(decompose_question(\"How did World War II affect the economy of the US?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1955b50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step-Back Example:\n",
      "1. What are the risks associated with using over-the-counter pain relievers in young children?\n",
      "2. What are the potential adverse effects of common pediatric medications?\n",
      "3. What safety considerations are important when administering medication to infants and toddlers?\n"
     ]
    }
   ],
   "source": [
    "# 4. STEP-BACK\n",
    "def step_back_question(question):\n",
    "    \"\"\"Generate a more general version of a question.\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Rewrite this question in 3 more general way:\\n{question}\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    return chain.run({\"question\": question})\n",
    "\n",
    "print(\"\\nStep-Back Example:\")\n",
    "print(step_back_question(\"What are the side effects of Paracetamol in children under 5?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efa24560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HyDE Example:\n",
      "Fake Doc: Photosynthesis is how plants use sunlight, water, and carbon dioxide to create their own food (sugar ...\n"
     ]
    }
   ],
   "source": [
    "# 5. HyDE\n",
    "def hyde_query(question):\n",
    "    \"\"\"Generate a hypothetical answer and embed it.\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Write a short hypothetical answer to this question:\\n{question}\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    fake_answer = chain.run({\"question\": question})\n",
    "    # Embed the fake answer with local embeddings\n",
    "    embedding = embedding_model.encode(fake_answer)\n",
    "    return embedding, fake_answer\n",
    "\n",
    "print(\"\\nHyDE Example:\")\n",
    "emb, fake_doc = hyde_query(\"Explain photosynthesis.\")\n",
    "print(\"Fake Doc:\", fake_doc[:100], \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
