{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9db93976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade google-genai python-dotenv --quiet\n",
    "# google-genai is Googleâ€™s GenAI Python SDK that talks to Gemini and the embedding models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2588f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv --quiet\n",
    "# python-dotenv is a library that loads environment variables from a .env file into the environment of the current process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17c9cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from google import genai\n",
    "from google.genai import types #helper types for safety\n",
    "# we will use the dotenv library to load the environment variables from the .env file.\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY is not set\")\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "22d68544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Token Count : 3\n"
     ]
    }
   ],
   "source": [
    "prompt_text = \"I like cake\"\n",
    "\n",
    "# Counting Tokens first. \n",
    "# Counting tokens helps you avoid exceeding model context limits and estimate cost.\n",
    "# We call the SDK's token counting method which uses the model's tokenizer.\n",
    "\n",
    "token_count = client.models.count_tokens(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents=[types.Content(parts=[types.Part(text=prompt_text)])]\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Total Token Count :\",getattr(token_count,\"total_tokens\",None))\n",
    "# gemini doesn't allow you to see the tokens generated by the model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41c464a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Embeddings created: 3072\n"
     ]
    }
   ],
   "source": [
    "# Create an internal embedding for the prompt\n",
    "embed_response = client.models.embed_content(\n",
    "    model=\"gemini-embedding-001\",                   \n",
    "    contents=[prompt_text],                         \n",
    "    config=types.EmbedContentConfig(\n",
    "        task_type=\"SEMANTIC_SIMILARITY\",            \n",
    "        output_dimensionality=3072                 \n",
    "    ),\n",
    ")\n",
    "\n",
    "embeddings = embed_response.embeddings[0].values\n",
    "print(\"Number of Embeddings created:\" ,len(embeddings))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77bc3232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic safety / moderation pre-check\n",
    "\n",
    "# You can instruct Gemini to use stricter safety thresholds for the request.\n",
    "# Here we set two categories to block low+ probability content (example).\n",
    "# If the input is disallowed the API may block the response or provide safety metadata.\n",
    "safety_settings = [\n",
    "    types.SafetySetting(\n",
    "        category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    types.SafetySetting(\n",
    "        category=types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5dea496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This `GenerateContentConfig` will be passed to the generator below.\n",
    "gen_config = types.GenerateContentConfig(\n",
    "    safety_settings=safety_settings,\n",
    "    # system_instruction can set the assistant behavior (optional)\n",
    "    system_instruction=\"You are a friendly assistant that replies in a very simple way and as if you're talking to a 10 year old with atleast 2 examples.\",\n",
    "    # set temperature=0 for deterministic outputs (optional)\n",
    "    temperature=0.7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "72994382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, that's awesome! Cake is super yummy!\n",
      "\n",
      "There are so many kinds of cake! Like, do you like **chocolate cake**? That's one kind! Or maybe you like a fun **vanilla cake with sprinkles**? That's another kind!\n"
     ]
    }
   ],
   "source": [
    "# Generate text with Gemini\n",
    "# We call generate_content with the same prompt. The API returns text and safety metadata.\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",                       # choose generation model\n",
    "    contents=[types.Content(parts=[types.Part(text=prompt_text)])],\n",
    "    config=gen_config\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7720c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Safety & Usage metadata ===\n",
      "Candidate 0 safety: None\n",
      "Usage metadata: cache_tokens_details=None cached_content_token_count=None candidates_token_count=57 candidates_tokens_details=None prompt_token_count=36 prompt_tokens_details=[ModalityTokenCount(\n",
      "  modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "  token_count=36\n",
      ")] thoughts_token_count=595 tool_use_prompt_token_count=None tool_use_prompt_tokens_details=None total_token_count=688 traffic_type=None\n"
     ]
    }
   ],
   "source": [
    "#Inspect safety ratings & usage metadata\n",
    "# The SDK returns fine-grained safety ratings and token usage to help you audit results.\n",
    "print(\"=== Safety & Usage metadata ===\")\n",
    "# prompt feedback: how the prompt rated under safety checks\n",
    "if getattr(response, \"prompt_feedback\", None):\n",
    "    print(\"Prompt feedback:\", response.prompt_feedback)\n",
    "\n",
    "# safety ratings for each candidate (if present)\n",
    "if getattr(response, \"safety_ratings\", None):\n",
    "    print(\"Safety ratings:\", response.safety_ratings)\n",
    "else:\n",
    "    # in some SDK variants the safety info lives under response.candidates[*].safetyRatings\n",
    "    for i, cand in enumerate(getattr(response, \"candidates\", [])):\n",
    "        print(f\"Candidate {i} safety:\", getattr(cand, \"safety_ratings\", None))\n",
    "\n",
    "# token usage metadata (helpful for cost estimation)\n",
    "usage = getattr(response, \"usage_metadata\", None)\n",
    "if usage:\n",
    "    print(\"Usage metadata:\", usage)\n",
    "else:\n",
    "    print(\"Usage metadata not present in this response object (varies by SDK version).\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
